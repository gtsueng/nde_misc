{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6e749b",
   "metadata": {},
   "source": [
    "## Duplicate record check\n",
    "\n",
    "Determining the extent of the duplicate records issue\n",
    "\n",
    "Zenodo versioning duplicates\n",
    "1. Pull name and id fields for 50,000 zenodo records\n",
    "2. Check for duplicate names on unique ids\n",
    "3. Calculate rate of duplication\n",
    "\n",
    "OmicsDI/GEO duplicates\n",
    "1. Pull name and id fields 1000 GEO records\n",
    "2. Search OMICS DI for matching names\n",
    "\n",
    "Zenodo/Dryad duplicates\n",
    "See OmicsDI/GEO duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f76575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "r = requests.get('https://api.data.niaid.nih.gov/v1/query?q=includedInDataCatalog.name:\"Zenodo\"&fields=name&fetch_all=true')\n",
    "cleanr = json.loads(r.text)\n",
    "hits = cleanr['hits']\n",
    "#print(len(cleanr['hits']))\n",
    "df1 = pd.DataFrame(cleanr['hits'])\n",
    "scroll_id = cleanr['_scroll_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 0\n",
    "while i < 10:\n",
    "    r2 = requests.get(f'https://api.data.niaid.nih.gov/v1/query?scroll_id={scroll_id}')\n",
    "    tmp = json.loads(r2.text)\n",
    "    scroll_id = tmp['_scroll_id']\n",
    "    tmpdf = pd.DataFrame(tmp['hits'])\n",
    "    df1 = pd.concat((df1,tmpdf),ignore_index=True)\n",
    "    print(len(df1))\n",
    "    i = i+1\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for replicated records (id and name)\n",
    "\n",
    "check_for_reps = df1.groupby(['_id','name']).size().reset_index(name='counts')\n",
    "replicates = check_for_reps.loc[check_for_reps['counts']>1]\n",
    "nonreps = check_for_reps.loc[check_for_reps['counts']==1]\n",
    "print(\"original length: \",len(df1),\" replicates: \",len(replicates))\n",
    "\n",
    "## Check for duplicate/version records (name only)\n",
    "check_for_dups = nonreps.groupby(['name']).size().reset_index(name='dup_counts')\n",
    "duplicates = check_for_dups.loc[check_for_dups['dup_counts']>1]\n",
    "nondups = check_for_dups.loc[check_for_dups['dup_counts']==1]\n",
    "\n",
    "## Stats\n",
    "{\"run\":n,\"samples\":len(df1),\"replicates\":len(replicates),\"duplicates\":len(duplicates),\"% dups\":len(duplicates)/len(replicates)*100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98652deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_zenodo_records(record_limit):\n",
    "    r = requests.get('https://api.data.niaid.nih.gov/v1/query?q=includedInDataCatalog.name:\"Zenodo\"&fields=name&fetch_all=true')\n",
    "    cleanr = json.loads(r.text)\n",
    "    hits = cleanr['hits']\n",
    "    #print(len(cleanr['hits']))\n",
    "    df1 = pd.DataFrame(cleanr['hits'])\n",
    "    scroll_id = cleanr['_scroll_id'] \n",
    "    i = 0\n",
    "    while i < record_limit:\n",
    "        r2 = requests.get(f'https://api.data.niaid.nih.gov/v1/query?scroll_id={scroll_id}')\n",
    "        tmp = json.loads(r2.text)\n",
    "        scroll_id = tmp['_scroll_id']\n",
    "        tmpdf = pd.DataFrame(tmp['hits'])\n",
    "        df1 = pd.concat((df1,tmpdf),ignore_index=True)\n",
    "        #print(len(df1))\n",
    "        i = i+1\n",
    "        time.sleep(0.5)  \n",
    "    return df1\n",
    "\n",
    "def check_dups(df1):\n",
    "    check_for_reps = df1.groupby(['_id','name']).size().reset_index(name='counts')\n",
    "    replicates = check_for_reps.loc[check_for_reps['counts']>1]\n",
    "    nonreps = check_for_reps.loc[check_for_reps['counts']==1]\n",
    "    check_for_dups = nonreps.groupby(['name']).size().reset_index(name='dup_counts')\n",
    "    duplicates = check_for_dups.loc[check_for_dups['dup_counts']>1]\n",
    "    nondups = check_for_dups.loc[check_for_dups['dup_counts']==1]\n",
    "    timecheck = datetime.now()\n",
    "    run_info = timecheck.strftime(\"%Y-%m-%d\")\n",
    "    tmpdict = {\"samples\":len(df1),\"replicates\":len(replicates),\n",
    "               \"duplicates\":len(duplicates),\"unique records\":len(nondups),\n",
    "               \"% dups\":len(duplicates)/len(nonreps)*100,\"run date\":run_info}\n",
    "    duplicates.to_csv(f\"duplicates_{run_info}.tsv\",sep='\\t',header=True)\n",
    "    return tmpdict\n",
    "\n",
    "def get_zenodo_dup_stats(repetitions, record_limit):\n",
    "    n = 0\n",
    "    statlist = []\n",
    "    while n < repetitions:\n",
    "        print(\"now performing run #\",n)\n",
    "        df1 = fetch_zenodo_records(record_limit)\n",
    "        tmpdict = check_dups(df1)\n",
    "        tmpdict['run number'] = n\n",
    "        statlist.append(tmpdict)\n",
    "        time.sleep(300)\n",
    "        n=n+1\n",
    "    return statlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now performing run # 0\n",
      "   samples  replicates  duplicates  unique records  % dups    run date  \\\n",
      "0    50000           0        2345           43819    4.69  2023-08-10   \n",
      "\n",
      "   run number  \n",
      "0           0  \n",
      "CPU times: total: 3.77 s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "repetitions = 1\n",
    "record_limit = 49\n",
    "statlist = get_zenodo_dup_stats(repetitions, record_limit)\n",
    "statdf = pd.DataFrame(statlist)\n",
    "statdf.to_csv('dup_stats.tsv')\n",
    "print(statdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4c409",
   "metadata": {},
   "source": [
    "## Checking Metadata differences between OMICS-DI and GEO\n",
    "\n",
    "1. Compare lengths of names and descriptions\n",
    "2. For duplicate records in this sample, pull 'species', 'measurementTechnique', and 'infectiousAgent' fields to compare the data from the two repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c559f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef472cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = read_csv('data/citation_df_clean.tsv',delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Find duplicate records\n",
    "## Since each record has a unique id, if we group by the name and citation pmid, we'll find duplicate records\n",
    "df3['pmid'] = df3['pmid'].astype(str)\n",
    "df3_counts = df3.groupby(['name','pmid']).size().reset_index(name='counts')\n",
    "rep_subset = df3_counts.loc[df3_counts['counts']>1]\n",
    "print(len(rep_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ef692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check to see if the number of unique names matches that of the number of unique citation records\n",
    "## Note, it does not. There are more unique names than pmids, therefore, some datasets cite the same pmid\n",
    "unique_names = rep_subset['name'].unique().tolist()\n",
    "unique_pmids = rep_subset['pmid'].unique().tolist()\n",
    "print(len(unique_names),len(unique_pmids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check to see if there are replicates (multiples of more than 2) \n",
    "print(df3_counts.sort_values('counts',ascending=False).head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using only name and pmid can result in multiple replicates. These may need special handling\n",
    "#### The issue of replicates may be due to both OMICS-DI ingestion of GEO and versioning\n",
    "#### First address the duplicates only as these will likely be due to OMICS-DI ingestion of GEO\n",
    "\n",
    "\n",
    "dup_freq_subset = rep_subset.loc[rep_subset['counts']<3]\n",
    "dup_subset = dup_freq_subset.merge(df3,on=['name','pmid'],how='left')\n",
    "print(dup_subset.head(n=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f022528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get pairs of ids\n",
    "## Sort the data frame by pmid (to get pairs), then by _id (to ensure orderting)\n",
    "## Generate one dataframe by dropping duplicates (subset pmid, keeping first)\n",
    "## Generate second dataframe by dropping duplicates (subset pmid, keeping list)\n",
    "## Merge the two to get pairs of data\n",
    "\n",
    "dup_subset.sort_values(by=['pmid','_id'], inplace=True)\n",
    "keep_first = dup_subset.drop_duplicates(subset='pmid',keep='first').copy()\n",
    "keep_last = dup_subset.drop_duplicates(subset='pmid',keep='last').copy()\n",
    "keep_first.rename(columns={'_id':'GEO_id','description':'GEO_desc'},inplace=True)\n",
    "keep_last.rename(columns={'_id':'OMICS_id','description':'OMICS_desc'},inplace=True)\n",
    "print(keep_first.head(n=5))\n",
    "print(\"===================\")\n",
    "print(keep_last.head(n=5))\n",
    "print(\"===================\")\n",
    "clean_dup_df = keep_first.merge(keep_last,on=['name','pmid','counts'],how='inner')\n",
    "print(clean_dup_df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_desc_length(row):\n",
    "    if row['GEO_desc_len'] > row['OMICS_desc_len']:\n",
    "        compare_result = 'GEO longer'\n",
    "    elif row['GEO_desc_len'] < row['OMICS_desc_len']:\n",
    "        compare_result = 'OMICS longer'\n",
    "    elif row['GEO_desc_len'] == row['OMICS_desc_len']:\n",
    "        compare_result = 'same length'\n",
    "    return compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d07be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare lengths of descriptions\n",
    "clean_dup_df['GEO_desc_len'] = clean_dup_df['GEO_desc'].str.len()\n",
    "clean_dup_df['OMICS_desc_len'] = clean_dup_df['OMICS_desc'].str.len()\n",
    "clean_dup_df['compare'] = clean_dup_df.apply(lambda row : compare_desc_length(row), axis = 1)\n",
    "print(clean_dup_df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_dup_df.iloc[0]['GEO_id'],clean_dup_df.iloc[0]['GEO_desc'])\n",
    "print('================================')\n",
    "print(clean_dup_df.iloc[0]['OMICS_id'],clean_dup_df.iloc[0]['OMICS_desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee06c0",
   "metadata": {},
   "source": [
    "### Summary of comparison of duplicate descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356dcc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarydf = clean_dup_df.groupby('compare').size()\n",
    "print(summarydf)\n",
    "\n",
    "rep_freq_subset = rep_subset.loc[rep_subset['counts']>3].copy()\n",
    "trip_freq_subset = rep_subset.loc[rep_subset['counts']==3].copy()\n",
    "\n",
    "print(\"replicates (>3): \", len(rep_freq_subset))\n",
    "print(\"triplicates (=3): \",len(trip_freq_subset))\n",
    "print(\"duplicates (=2): \", len(dup_freq_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890202c5",
   "metadata": {},
   "source": [
    "Issue of replicates and triplicates seems to primarily be due to the use of a species name as the name of the dataset. These types of datasets are likely to cite the same PMID paper describing the species and may consist of wholly different datasets based on the descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f464ba5",
   "metadata": {},
   "source": [
    "### Investigate source of triplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356471e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trip_subset = trip_freq_subset.merge(df3,on=['name','pmid'],how='left')\n",
    "trip_subset.sort_values(by=['pmid','name'],inplace=True)\n",
    "trip_subset.to_csv('data/triplicates_by_name_and_pmid.tsv', sep='\\t',header=True)\n",
    "print(trip_subset.head(n=21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963c20b",
   "metadata": {},
   "source": [
    "### Identify a heuristic for ommitting replicates based on name length or match to a species name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2accec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspecting name lengths\n",
    "rep_freq_subset['name_length'] = rep_freq_subset['name'].str.len()\n",
    "rep_freq_subset.sort_values(by='name_length',ascending=True,inplace=True)\n",
    "rep_name_mean = rep_freq_subset['name_length'].mean()\n",
    "rep_name_min = rep_freq_subset['name_length'].min() \n",
    "rep_name_max = rep_freq_subset['name_length'].max()\n",
    "print(\"replicates: \", \"min: \", rep_name_min, \"max: \", rep_name_max, \"mean: \", rep_name_mean)\n",
    "\n",
    "trip_freq_subset['name_length'] = trip_freq_subset['name'].str.len()\n",
    "trip_freq_subset.sort_values(by='name_length',ascending=True,inplace=True)\n",
    "trip_name_mean = trip_freq_subset['name_length'].mean()\n",
    "trip_name_min = trip_freq_subset['name_length'].min() \n",
    "trip_name_max = trip_freq_subset['name_length'].max()\n",
    "print(\"triplicates: \", \"min: \", trip_name_min, \"max: \", trip_name_max, \"mean: \", trip_name_mean)\n",
    "\n",
    "dup_freq_subset['name_length'] = dup_freq_subset['name'].str.len()\n",
    "dup_freq_subset.sort_values(by='name_length',ascending=True,inplace=True)\n",
    "dup_name_mean = dup_freq_subset['name_length'].mean()\n",
    "dup_name_min = dup_freq_subset['name_length'].min() \n",
    "dup_name_max = dup_freq_subset['name_length'].max()\n",
    "print(\"duplicates: \", \"min: \", dup_name_min, \"max: \", dup_name_max, \"mean: \", dup_name_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rep_freq_subset.head(n=3))\n",
    "\n",
    "print(trip_freq_subset.head(n=3))\n",
    "\n",
    "print(dup_freq_subset.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test = [25, 50, 75, 90]\n",
    "\n",
    "for eachcutoff in cutoff_test:\n",
    "    tmprepdf = rep_freq_subset.loc[rep_freq_subset['name_length']>eachcutoff]\n",
    "    tmprepdf.sort_values('name_length',ascending=True,inplace=True)\n",
    "    print(\"reps at \"+str(eachcutoff),\": \",tmprepdf.head(n=3))\n",
    "    tmptripdf = trip_freq_subset.loc[trip_freq_subset['name_length']>eachcutoff]\n",
    "    tmptripdf.sort_values('name_length',ascending=True,inplace=True)\n",
    "    print(\"trips at \"+str(eachcutoff),\": \",tmptripdf.head(n=3))\n",
    "    tmpdupdf = dup_freq_subset.loc[dup_freq_subset['name_length']>eachcutoff]\n",
    "    tmpdupdf.sort_values('name_length',ascending=True,inplace=True)\n",
    "    print(\"trips at \"+str(eachcutoff),\": \",tmpdupdf.head(n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.head(n=2))\n",
    "replicatesdf = df3.merge(tmprepdf, on=['name','pmid'], how='inner')\n",
    "print(len(replicatesdf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
