{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d885e014",
   "metadata": {},
   "source": [
    "## measurementTechnique extraction and standardization review\n",
    "\n",
    "#### Analyze the extraction of measurementTechniques by ChatGPT\n",
    "Approach:\n",
    "- Select 10 records at random from each of the sampled repositories\n",
    "- For each record, evaluate the techniques pulled by ChatGPT as True or False positive based on description\n",
    "- Calculate precision and recall for each record\n",
    "- If there are terms which are the same, but appear different, pull those out for standardization analysis\n",
    "\n",
    "#### Analyse the extraction of extraneous generic 'stop word' terms\n",
    "Approach:\n",
    "- Split all ChatGPT generated terms into words (split by space)\n",
    "- Generate frequency table of terms\n",
    "- Identify top set of generic terms for use as permutations in standardization\n",
    "\n",
    "#### Analyze the standardization of measurementTechnique terms\n",
    "Approach:\n",
    "- Create list of terms based based on mapping to multiple ontologies\n",
    "- Create permutations in term list\n",
    "- Run the standardization approach and calculate precision, recall F relative to original term\n",
    "\n",
    "#### Determine threshold for cut off to favor false negatives over false positives\n",
    "Approach:\n",
    "- For each threshhold cutoff\n",
    "  - Select 25 terms and determine how well they matched\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f8fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d387e30",
   "metadata": {},
   "source": [
    "### Analyze ChatGPT's extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the file\n",
    "gpt_results = pd.read_csv(os.path.join(data_path,'GPT Measurement Techniques results.tsv'),delimiter='\\t',header=0)\n",
    "print(gpt_results.head(n=2))\n",
    "\n",
    "## select 10 random records\n",
    "ransamps = gpt_results.groupby('Data Repository').sample(10)\n",
    "\n",
    "def ifenumed(rowdata):\n",
    "    numlist = ['1.','2.','3.','4.','5.','6.','7.','8.','9.','10.','11.','12.','13.','14.','15.']\n",
    "    for eachnum in numlist:\n",
    "        if eachnum in rowdata:\n",
    "            rowdata.replace(eachnum,\" - \")\n",
    "        else:\n",
    "            break\n",
    "    return rowdata\n",
    "\n",
    "## format results from records\n",
    "def clean_predictions(row):\n",
    "    rowdata = row['Predictions']\n",
    "    if '1. ' in rowdata:\n",
    "        rowdata = ifenumed(rowdata)\n",
    "    tmpdata = rowdata.replace(\" - \",\"|\")\n",
    "    tmplist = tmpdata.split(\"|\")\n",
    "    cleanlist = [x.replace(\"- \",\"\").strip() for x in tmplist]\n",
    "    return cleanlist \n",
    "\n",
    "ransamps['clean_pred'] = ransamps.apply(lambda row: clean_predictions(row), axis=1)\n",
    "exploded_df = ransamps.explode('clean_pred')\n",
    "#print(ransamps.head(n=2))\n",
    "exploded_df.drop(columns=['Model','Predictions'],inplace=True)\n",
    "print(exploded_df.head(n=2))\n",
    "\n",
    "## Export results for manual evaluation\n",
    "exploded_df.to_csv(os.path.join(result_path,'GPT_sample.tsv'),sep='\\t',header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504a9d6",
   "metadata": {},
   "source": [
    "### Analyse the extraction of extraneous generic 'stop word' terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b28a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull the terms from Dylan's list (since the source is not needed)\n",
    "t2t_results = pd.read_csv(os.path.join(data_path,'Measurement Techniques mapped.tsv'),delimiter='\\t',header=2)\n",
    "\n",
    "## Pull the list of techniques\n",
    "techniques = t2t_results['Technique'].tolist()\n",
    "\n",
    "## process the terms\n",
    "cleanlist = []\n",
    "for eachterm in techniques:\n",
    "    tmpterm = eachterm.lower()\n",
    "    tmpterm.replace(\"-\",\" \").replace(\":\",\" \")\n",
    "    tmplist = tmpterm.split(\" \")\n",
    "    cleantmp = [x.replace(\"(\",\"\").replace(\")\",\"\").strip() for x in tmplist]\n",
    "    cleanlist.extend(cleantmp)\n",
    "\n",
    "termseries = pd.Series(cleanlist)\n",
    "termdf = termseries.to_frame('technique')\n",
    "termfreq = termdf.groupby('technique').size().reset_index(name='counts')\n",
    "termfreq.sort_values('counts',ascending=False,inplace=True)\n",
    "print(termfreq.loc[termfreq['counts']>5])\n",
    "termfreq.to_csv(os.path.join(result_path,'stopword_freq.tsv'),sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94d669",
   "metadata": {},
   "source": [
    "### Analyze the standardization of measurementTechnique terms\n",
    "\n",
    "#### Get measurementTechnique terms to test\n",
    "\n",
    "* mappings endpoint documentation: https://data.bioontology.org/documentation#Mapping\n",
    "* sample code: https://gist.github.com/callahantiff/a28fb3160782f42f104e9ec41553af0d\n",
    "* NCBO sample code: https://github.com/ncbo/ncbo_rest_sample_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "\n",
    "## Load the API key\n",
    "with open(os.path.join(script_path,'config.json'),'rb') as keyfile:\n",
    "    keyinfo = json.load(keyfile)\n",
    "    apikey = keyinfo['apikey']\n",
    "\n",
    "## Format the apikey for the header\n",
    "def get_json(url, apikey):\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + apikey)]\n",
    "    return json.loads(opener.open(url).read())\n",
    "\n",
    "## Provide the list of ontologies to map\n",
    "onto_list = [\"BAO\",\"OBI\",\"EFO\",\"NCIT\",\"EDAMT\",\"MMO\",\"CHMO\"]\n",
    "\n",
    "## Pull mapped pairs out of a paginated dictionary\n",
    "def get_mappings(onto_source,page_dict):\n",
    "    mappinglist = []\n",
    "    for eachcollection in page_dict['collection']:\n",
    "        tmpdict = {'source_ontology': onto_source,\n",
    "                   'source_id': eachcollection['classes'][0]['@id'],\n",
    "                   'map_method': eachcollection['source'],\n",
    "                   'target_id': eachcollection['classes'][1]['@id']}\n",
    "        mappinglist.append(tmpdict)\n",
    "    return mappinglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter the results to only mappings within ontologies of interest\n",
    "def filter_for_ontos(onto_list,mappingdf):\n",
    "    relevant_df = pd.DataFrame(columns=mappingdf.columns.tolist())\n",
    "    source_ont = mappingdf.iloc[0]['source_ontology']\n",
    "    target_list = [x for x in onto_list if x!=source_ont]\n",
    "    for eachtarget in target_list:\n",
    "        target_subset = mappingdf.loc[mappingdf['target_id'].str.contains(eachtarget)]\n",
    "        relevant_df = pd.concat(([relevant_df,target_subset]),ignore_index=True)\n",
    "    return relevant_df\n",
    "\n",
    "def download_mappings(apikey,onto_list,starting_page):\n",
    "    for each_onto in onto_list:\n",
    "        print(\"now downloading mappings from: \",each_onto)\n",
    "        allmappinglist = []\n",
    "        ontologymap = f\"https://data.bioontology.org/ontologies/{each_onto}/mappings\"\n",
    "        r = get_json(ontologymap,apikey)\n",
    "        if starting_page == 0:\n",
    "            page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "        else:\n",
    "            page = starting_page\n",
    "        next_page = page\n",
    "        while next_page:\n",
    "            next_page = page[\"links\"][\"nextPage\"]\n",
    "            tmpmapping = get_mappings(each_onto, page)\n",
    "            allmappinglist.extend(tmpmapping)\n",
    "            if next_page:\n",
    "                page = get_json(next_page,apikey)\n",
    "                print(page[\"links\"][\"nextPage\"])\n",
    "        mappingdf = pd.DataFrame(allmappinglist)\n",
    "        relevant_df = filter_for_ontos(onto_list,mappingdf)\n",
    "        relevant_df.to_csv(os.path.join(result_path,\"mappings\",f\"{each_onto}_mappings.tsv\"), sep='\\t', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3074b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Download all mappings from ontologies of interest\n",
    "download_mappings(apikey,onto_list,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa547328",
   "metadata": {},
   "source": [
    "### Figuring out the data structure of a result from the API\n",
    "\n",
    "The organization of the mappings appear to be as follows:\n",
    "A result from the API ontology/mappings endpoint:\n",
    "* r.keys:  dict_keys(['page', 'pageCount', 'totalCount', 'prevPage', 'nextPage', 'links', 'collection'])\n",
    "  * r['collection'].keys():  dict_keys(['id', 'source', 'classes', 'process', '@id', '@type'])\n",
    "    * r['collection'][0]['classes'].keys:  dict_keys(['@id', '@type', 'links', '@context'])\n",
    "\n",
    "Where each pair of mapped terms appear to be listed under 'classes' with classes[0] being the term in the source ontology and classes[1] being the term from a different ontology\n",
    "\n",
    "The source is how two terms were mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62945cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test to see that an API request is working\n",
    "REST_URL = \"http://data.bioontology.org\"\n",
    "term = \"survey\"\n",
    "r = get_json(REST_URL + \"/search?q=\" + term,apikey)[\"collection\"]\n",
    "#print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the class mapping endpoint\n",
    "ontology_shorthand = 'BRO'\n",
    "classurl = 'http%3A%2F%2Fbioontology.org%2Fontologies%2FBiomedicalResourceOntology.owl%23Ontology_Development_and_Management'\n",
    "classmap = f\"https://data.bioontology.org/ontologies/{ontology_shorthand}/classes/{classurl}/mappings\"\n",
    "r = get_json(classmap,apikey)\n",
    "print('r[0].keys: ', r[0].keys())\n",
    "print('r[0][classes][0].keys: ', r[0]['classes'][0].keys())\n",
    "print('r[0][classes][0][links]: ', r[0]['classes'][0]['links'].keys())\n",
    "print(r[0]['classes'][0]['links']['descendants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the ontology mapping end point\n",
    "ontology_shorthand = 'MMO'\n",
    "classurl = 'http://purl.obolibrary.org/obo/MMO_0000000'\n",
    "ontologymap = f\"https://data.bioontology.org/ontologies/{ontology_shorthand}/mappings\"\n",
    "r = get_json(ontologymap,apikey)\n",
    "\n",
    "print(\"r.keys: \",r.keys())\n",
    "print(\"r['links']: \", r['links'])\n",
    "print(\"r['page']: \", r['page'])\n",
    "print(\"r['collection'].keys(): \", r['collection'][0].keys())\n",
    "print(\"r['collection'][0]['classes'].keys: \", r['collection'][0]['classes'][0].keys())\n",
    "print(\"collection id: \",r['collection'][0][\"@id\"])\n",
    "print(\"class id: \", r['collection'][0]['classes'][1][\"@id\"])\n",
    "print(\"class type: \", r['collection'][0]['classes'][1][\"@type\"])\n",
    "print(\"class context: \", r['collection'][0]['classes'][1][\"@context\"])\n",
    "print(\"number of classes: \",len(r['collection'][0]['classes']))\n",
    "\n",
    "print(r['pageCount'])\n",
    "for eachcollection in r['collection'][0:3]:\n",
    "    #print(len(eachcollection['classes']))\n",
    "    print(eachcollection['classes'][0]['@id'],eachcollection['source'],eachcollection['classes'][1]['@id'])\n",
    "    print(eachcollection['classes'][0]['@context'],eachcollection['classes'][1]['@context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e16cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the use of pagination\n",
    "page = get_json(r['links'][\"nextPage\"],apikey)\n",
    "allmappinglist = []\n",
    "# Iterate over the available pages adding labels from all classes\n",
    "# When we hit the last page, the while loop will exit\n",
    "next_page = page\n",
    "while next_page:\n",
    "    next_page = page[\"links\"][\"nextPage\"]\n",
    "    tmpmapping = get_mappings(\"MMO\", page)\n",
    "    allmappinglist.extend(tmpmapping)\n",
    "    if next_page:\n",
    "        page = get_json(next_page,apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f391d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingdf = pd.DataFrame(mappinglist)\n",
    "print(mappingdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mappingdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8d2fc",
   "metadata": {},
   "source": [
    "## Pulling terms that are mapped between measTech Ontologies\n",
    "\n",
    "MMO and CHMO are more technique-focused ontologies, so focusing on the mappings between these ontologies and others is a fast way to obtain measurementTechnique terms. \n",
    "\n",
    "### Pull terms for mapping via T2T pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')\n",
    "onto_path = os.path.join(data_path,'ontology_files')\n",
    "map_path = os.path.join(result_path,'mappings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMO_mapping = pd.read_csv(os.path.join(map_path,'MMO_mappings.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "CHMO_mapping = pd.read_csv(os.path.join(map_path,'CHMO_mappings.tsv'),delimiter='\\t',header=0,index_col=0) \n",
    "mapping_list = [MMO_mapping,CHMO_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed693f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull synonymous terms based on the mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMO_onto = pd.read_csv(os.path.join(onto_path,'MMO.csv'),delimiter=',',header=0, usecols=['Class ID','Preferred Label','Synonyms','has_exact_synonym'])\n",
    "MMO_onto['alternative term'] = 'N/A'\n",
    "CHMO_onto = pd.read_csv(os.path.join(onto_path,'CHMO.csv'),delimiter=',',header=0, usecols=['Class ID','Preferred Label','Synonyms','has_exact_synonym'])\n",
    "CHMO_onto['alternative term'] = 'N/A'\n",
    "EFO_onto = pd.read_csv(os.path.join(onto_path,'EFO.csv'),delimiter=',',header=0, usecols=['Class ID','Preferred Label','Synonyms','has_exact_synonym'])\n",
    "EFO_onto['alternative term'] = 'N/A'\n",
    "OBI_onto = pd.read_csv(os.path.join(onto_path,'OBI.csv'),delimiter=',',header=0, usecols=['Class ID','Preferred Label','Synonyms','alternative term'])\n",
    "OBI_onto['has_exact_synonym'] = 'N/A'\n",
    "BAO_onto = pd.read_csv(os.path.join(onto_path,'BAO.csv'),delimiter=',',header=0, usecols=['Class ID','Preferred Label','Synonyms','alternative term'])\n",
    "BAO_onto['has_exact_synonym'] = 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MMO_mapping.head(n=2))\n",
    "\n",
    "ontodict = {'MMO':MMO_onto, 'EFO':EFO_onto, 'CHMO':CHMO_onto, 'BAO':BAO_onto, 'OBI':OBI_onto}\n",
    "classlist = []\n",
    "for eachmapping in mapping_list:\n",
    "    sourcelist = eachmapping['source_id'].unique().tolist()\n",
    "    targetlist = MMO_mapping['target_id'].unique().tolist()\n",
    "    classlist.extend(sourcelist)\n",
    "    classlist.extend(targetlist)\n",
    "    classlist = list(set(classlist))\n",
    "\n",
    "print(len(classlist), classlist[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "termdf = pd.DataFrame(columns = ['Class ID','Preferred Label','Synonyms','alternative term','has_exact_synonym'])\n",
    "\n",
    "namespace_list = ['MMO','CHMO','BAO','OBI','EFO']\n",
    "\n",
    "for eachnamespace in namespace_list:\n",
    "    tmpclasslist = [x for x in classlist if 'eachnamespace' in x]\n",
    "    eachdf = ontodict[eachnamespace]\n",
    "    tmpdf = eachdf.loc[eachdf['Class ID'].isin(classlist)]\n",
    "    termdf = pd.concat((termdf,tmpdf),ignore_index=True)\n",
    "\n",
    "print(termdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e46981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_synonyms(a_synonym):\n",
    "    try:\n",
    "        syns = a_synonym.split('|')\n",
    "    except:\n",
    "        syns = 'N/A'\n",
    "    return syns\n",
    "\n",
    "termdf['synlist'] = termdf.apply(lambda row: split_synonyms(row['Synonyms']),axis=1)\n",
    "expanded_terms = termdf.explode('synlist')\n",
    "\n",
    "print(expanded_terms.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07705786",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_terms = expanded_terms['Preferred Label'].unique().tolist()\n",
    "synterms = expanded_terms['synlist'].unique().tolist()\n",
    "all_terms = list(set(preferred_terms).union(set(synterms)))\n",
    "print(len(all_terms),all_terms[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mutate the terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b557c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = ['analysis','study','testing','sampling','assessment','diagnostic','tests',\n",
    "                 'design','review','detection','identification','administration','system',\n",
    "                 'surveillance','process','approach','method']\n",
    "\n",
    "def add_stopword(original_term,stopword_list):\n",
    "    stopword_len = len(stopword_list)\n",
    "    doublestop = 2*stopword_len\n",
    "    tmp_num = random.randrange(0,doublestop)\n",
    "    if tmp_num < stopword_len:\n",
    "        tmpterm = original_term +\" \"+ stopword_list[tmp_num]\n",
    "    else:\n",
    "        tmpterm = original_term\n",
    "    return tmpterm\n",
    "\n",
    "all_term_df = pd.DataFrame(all_terms)\n",
    "all_term_df.rename(columns={0:'Original term'},inplace=True)\n",
    "all_term_df['Syn1'] = all_term_df.apply(lambda row: add_stopword(row['Original term'],stopword_list),axis=1)\n",
    "all_term_df['Syn2'] = all_term_df.apply(lambda row: add_stopword(row['Original term'],stopword_list),axis=1)\n",
    "all_term_df['Syn3'] = all_term_df.apply(lambda row: add_stopword(row['Original term'],stopword_list),axis=1)\n",
    "print(all_term_df.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_term_df.to_csv(os.path.join(result_path,'mmo_chmo_terms.tsv'),sep='\\t',header=True)\n",
    "expanded_terms.to_csv(os.path.join(result_path,'mmo_chmo_terms_mapped.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_2_check = list(set(all_terms).union(set(all_term_df['Syn1'].unique().tolist()).union(set(all_term_df['Syn2']).union(set(all_term_df['Syn3'].unique().tolist())))))\n",
    "with open(os.path.join(result_path,'terms_2_test.txt'),'w') as outwrite:\n",
    "    for eachterm in terms_2_check:\n",
    "        outwrite.write(eachterm+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b90fac4",
   "metadata": {},
   "source": [
    "### Format the results of the terms mapped via T2T pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fe258",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_result = pd.read_csv(os.path.join(data_path,'measTech_T2T_test_result.tsv'), delimiter='\\t',header=0)\n",
    "t2t_result.rename(columns={'Term.1':'found_term'},inplace=True)\n",
    "print(t2t_result.head(n=2))\n",
    "print(len(t2t_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d187ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_term_df = pd.read_csv(os.path.join(result_path,'mmo_chmo_terms.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(original_term_df.head(n=2))\n",
    "original_term_map = pd.read_csv(os.path.join(result_path,'mmo_chmo_terms_mapped.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(original_term_map.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f17991",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn1 = original_term_df[['Original term','Syn1']].copy()\n",
    "syn1.rename(columns = {'Syn1':'Term'},inplace=True)\n",
    "syn1_merged = syn1.merge(t2t_result, on='Term',how='inner')\n",
    "syn2 = original_term_df[['Original term','Syn2']].copy()\n",
    "syn2.rename(columns = {'Syn2':'Term'},inplace=True)\n",
    "syn2_merged = syn2.merge(t2t_result, on='Term',how='inner')\n",
    "syn3 = original_term_df[['Original term','Syn3']].copy()\n",
    "syn3.rename(columns = {'Syn3':'Term'},inplace=True)\n",
    "syn3_merged = syn3.merge(t2t_result, on='Term',how='inner')\n",
    "\n",
    "#print(len(syn1_merged), syn1_merged.head(n=2))\n",
    "#print(len(syn2_merged), syn2_merged.head(n=2))\n",
    "#print(len(syn3_merged), syn3_merged)\n",
    "\n",
    "all_terms = pd.concat((syn1_merged,syn2_merged,syn3_merged),ignore_index=True)\n",
    "all_terms.drop_duplicates(keep='first')\n",
    "print(len(all_terms))\n",
    "print(all_terms.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57328e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferred_term = original_term_map[['Class ID','Preferred Label']].copy()\n",
    "preferred_term.rename(columns={'Preferred Label':'Original term'},inplace=True)\n",
    "\n",
    "synonym_term = original_term_map[['Class ID','synlist']].copy()\n",
    "synonym_term.rename(columns={'synlist':'Original term'},inplace=True)\n",
    "\n",
    "preferred_merge = preferred_term.merge(all_terms,on='Original term',how='inner')\n",
    "print(len(preferred_merge))\n",
    "print(preferred_merge.head(n=2))\n",
    "print(preferred_merge.iloc[0]['Term'],preferred_merge.iloc[1]['Term'] )\n",
    "synonym_merge = synonym_term.merge(all_terms,on='Original term',how='inner')\n",
    "print(len(synonym_merge))\n",
    "\n",
    "results_to_analyze = pd.concat((preferred_merge,synonym_merge),ignore_index=True)\n",
    "#results_to_analyze.to_csv(os.path.join(result_path,'T2T_results_formatted.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect number of original (preferred label) terms tested\n",
    "preferred_terms_tested = preferred_merge.groupby('Original term')\n",
    "print(len(preferred_terms_tested))\n",
    "\n",
    "synonym_term_tested  = synonym_merge.groupby('Original term')\n",
    "print(len(synonym_term_tested))\n",
    "\n",
    "terms_tested = results_to_analyze.groupby('Term')\n",
    "print(len(terms_tested))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066c199",
   "metadata": {},
   "source": [
    "## Analyze the formatted results of the T2T pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c52f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')\n",
    "\n",
    "results_to_analyze = pd.read_csv(os.path.join(result_path,'T2T_results_formatted.tsv'),delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48037650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_urls(bao_url):\n",
    "    if \"bao\" in bao_url:\n",
    "        if \"ebi\" in bao_url:\n",
    "            clean_url = bao_url.replace(\"https://www.ebi.ac.uk/ols4/ontologies/bao/classes/http%253A%252F%252Fwww.bioassayontology.org%252Fbao%2523\",\"http://www.bioassayontology.org/bao#\")\n",
    "        else:\n",
    "            clean_url = bao_url\n",
    "    else:\n",
    "        clean_url = bao_url\n",
    "    return clean_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "342a75d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Class ID  \\\n",
      "0  http://purl.obolibrary.org/obo/MMO_0000380   \n",
      "1  http://purl.obolibrary.org/obo/MMO_0000380   \n",
      "\n",
      "                                     Original term  \\\n",
      "0  graphite furnace atomic absorption spectrometry   \n",
      "1  graphite furnace atomic absorption spectrometry   \n",
      "\n",
      "                                                Term  T2T best score Ontology  \\\n",
      "0  graphite furnace atomic absorption spectrometr...           0.834      mmo   \n",
      "1  graphite furnace atomic absorption spectrometr...           0.883      mmo   \n",
      "\n",
      "                                        found_term  \\\n",
      "0  graphite furnace atomic absorption spectrometry   \n",
      "1  graphite furnace atomic absorption spectrometry   \n",
      "\n",
      "                                       TermID  \n",
      "0  http://purl.obolibrary.org/obo/MMO_0000380  \n",
      "1  http://purl.obolibrary.org/obo/MMO_0000380  \n",
      "307\n"
     ]
    }
   ],
   "source": [
    "results_to_analyze['TermID'] = results_to_analyze.apply(lambda row: clean_up_urls(row['TermID']), axis=1)\n",
    "print(results_to_analyze.head(n=2))\n",
    "exact_matches = results_to_analyze.loc[results_to_analyze['Class ID'] == results_to_analyze['TermID']].copy()\n",
    "exact_matches.drop_duplicates(keep=\"first\",inplace=True)\n",
    "print(len(exact_matches))\n",
    "#exact_matches.to_csv(os.path.join(result_path,'T2T_exact_matches.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f48c93",
   "metadata": {},
   "source": [
    "#### Handler for BAO Class ID's since the TermID may use an OLS version of BAO\n",
    "\n",
    "* Example of BAO ID from NCBO BioPortal Mappings: http://www.bioassayontology.org/bao#BAO_0000415\n",
    "* Example of BAO ID from T2T results: https://www.ebi.ac.uk/ols4/ontologies/bao/classes/http%253A%252F%252Fwww.bioassayontology.org%252Fbao%2523BAO_0000453\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3fe271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2T best score  (0.0, 0.5]  (0.5, 0.6]  (0.6, 0.7]  (0.7, 0.8]  (0.8, 0.9]  \\\n",
      "Ontology                                                                     \n",
      "bao                      0           0           0           7           8   \n",
      "chmo                     0           0           5          24          58   \n",
      "efo                      0           0           0           1           0   \n",
      "mmo                      0           1           3          21          33   \n",
      "obi                      0           0           1           6           7   \n",
      "\n",
      "T2T best score  (0.9, 1.0]  \n",
      "Ontology                    \n",
      "bao                      7  \n",
      "chmo                    72  \n",
      "efo                      2  \n",
      "mmo                     41  \n",
      "obi                     10  \n"
     ]
    }
   ],
   "source": [
    "## Exact matches based on identifier\n",
    "score_bins = [0,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "groups = exact_matches.groupby(['Ontology', pd.cut(exact_matches['T2T best score'], score_bins)],observed=False)\n",
    "print(groups.size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7702b993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "## Exact matches of manipulated terms\n",
    "original_term_exact = exact_matches.loc[exact_matches['Original term'] == exact_matches['Term']]\n",
    "#original_term_exact = exact_matches.loc[exact_matches['Original term'] == exact_matches['Term']]['Original term'].unique()\n",
    "#manipulated_term_match = exact_matches.loc[exact_matches['Original term'] != exact_matches['Term']]\n",
    "manipulated_term_match = exact_matches.loc[exact_matches['Original term'] != exact_matches['Term']]['Original term'].unique()\n",
    "print(len(original_term_exact))\n",
    "print(len(manipulated_term_match))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "783c5c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mismatched = results_to_analyze.loc[results_to_analyze['Class ID'] != results_to_analyze['TermID']].copy()\n",
    "mismatched.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9edaf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "59\n",
      "52\n",
      "137\n",
      "T2T best score  (0.0, 0.5]  (0.5, 0.6]  (0.6, 0.7]  (0.7, 0.8]  (0.8, 0.9]  \\\n",
      "Ontology                                                                     \n",
      "bao                      0           0           0          18          30   \n",
      "chmo                     0           0           0           0           3   \n",
      "efo                      0           0           0           3           5   \n",
      "mmo                      0           0           2           4          22   \n",
      "obi                      0           0           0           0           4   \n",
      "\n",
      "T2T best score  (0.9, 1.0]  \n",
      "Ontology                    \n",
      "bao                     33  \n",
      "chmo                     2  \n",
      "efo                      9  \n",
      "mmo                     17  \n",
      "obi                      2  \n"
     ]
    }
   ],
   "source": [
    "## of exact matches by term\n",
    "exact_term_matches = mismatched.loc[mismatched['Original term'] == mismatched['found_term']].copy()\n",
    "exact_term_matches.drop_duplicates(keep='first',inplace=True)\n",
    "#exact_term_matches.to_csv(os.path.join(result_path,'T2T_term_matches.tsv'),sep='\\t',header=True)\n",
    "original_exact = exact_term_matches.loc[exact_term_matches['Original term'] == exact_term_matches['Term']]\n",
    "\n",
    "print(len(exact_term_matches))\n",
    "print(len(original_exact))\n",
    "print(len(original_exact['Term'].unique().tolist()))\n",
    "print(len(exact_term_matches['Term'].unique().tolist()))\n",
    "\n",
    "score_bins = [0,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "groups = exact_term_matches.groupby(['Ontology', pd.cut(exact_term_matches['T2T best score'], score_bins)],observed=False)\n",
    "print(groups.size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b4df156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "295\n",
      "257\n",
      "785\n",
      "T2T best score  (0.0, 0.5]  (0.5, 0.6]  (0.6, 0.7]  (0.7, 0.8]  (0.8, 0.9]  \\\n",
      "Ontology                                                                     \n",
      "bao                      2           1          20          18          15   \n",
      "chmo                     0          14          18          12          17   \n",
      "edam                     0           0           0          13          19   \n",
      "efo                      0           0           1           2          10   \n",
      "mmo                      0           0           3          14          18   \n",
      "ncit                     8          35         102         190         533   \n",
      "obi                      0           2           5           7           8   \n",
      "\n",
      "T2T best score  (0.9, 1.0]  \n",
      "Ontology                    \n",
      "bao                     14  \n",
      "chmo                    13  \n",
      "edam                    15  \n",
      "efo                      5  \n",
      "mmo                     16  \n",
      "ncit                   126  \n",
      "obi                      4  \n"
     ]
    }
   ],
   "source": [
    "## True mismatches by ID and term\n",
    "true_mismatch = mismatched.loc[mismatched['Original term'] != mismatched['found_term']].copy()\n",
    "original_true_mismatch = true_mismatch.loc[true_mismatch['Original term'] == true_mismatch['Term']]\n",
    "true_mismatch.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "print(len(true_mismatch))\n",
    "print(len(original_true_mismatch))\n",
    "print(len(original_true_mismatch['Term'].unique().tolist()))\n",
    "print(len(true_mismatch['Term'].unique().tolist()))\n",
    "\n",
    "score_bins = [0,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "groups = true_mismatch.groupby(['Ontology', pd.cut(true_mismatch['T2T best score'], score_bins)],observed=False)\n",
    "print(groups.size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aed8c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "985\n",
      "257\n",
      "309\n"
     ]
    }
   ],
   "source": [
    "## matches of manipulated terms\n",
    "original_term_mismatch = true_mismatch.loc[true_mismatch['Original term'] == true_mismatch['Term']]\n",
    "manipulated_term_mismatch = true_mismatch.loc[true_mismatch['Original term'] != true_mismatch['Term']]\n",
    "print(len(original_term_mismatch))\n",
    "print(len(manipulated_term_mismatch))\n",
    "\n",
    "original_term_mismatch = true_mismatch.loc[true_mismatch['Original term'] == true_mismatch['Term']]['Original term'].unique()\n",
    "manipulated_term_mismatch = true_mismatch.loc[true_mismatch['Original term'] != true_mismatch['Term']]['Original term'].unique()\n",
    "print(len(original_term_mismatch))\n",
    "print(len(manipulated_term_mismatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a2a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Review matches of manipulated terms by ontology\n",
    "ontolist = ['bao','chmo','edam','efo','mmo','ncit','obi']\n",
    "\n",
    "for eachonto in ontolist:\n",
    "    tmpdf = true_mismatch.loc[true_mismatch['Ontology']==eachonto].copy()\n",
    "    tmpdf.drop_duplicates(keep='first',inplace=True)\n",
    "    tmpdf.to_csv(os.path.join(result_path,'to_review',f'mismatch_{eachonto}.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1be39",
   "metadata": {},
   "source": [
    "## Analyze the T2T terms that did not match via ID or exact word matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b81cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')\n",
    "file_path = os.path.join(data_path,'reviewed_mismatched_terms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4788c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsueng\\AppData\\Local\\Temp\\ipykernel_13344\\4044624005.py:7: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  basedf = pd.concat((basedf,tmp),ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Class ID Original term  \\\n",
      "0  http://purl.obolibrary.org/obo/MMO_0000496     histology   \n",
      "1  http://purl.obolibrary.org/obo/MMO_0000496     histology   \n",
      "\n",
      "                Term  T2T best score  \\\n",
      "0  histology testing           0.747   \n",
      "1          histology           0.996   \n",
      "\n",
      "                                       TermID Ontology found_term match_eval  \n",
      "0  http://purl.obolibrary.org/obo/NCIT_C16681      bao  Histology       good  \n",
      "1  http://purl.obolibrary.org/obo/NCIT_C16681      bao  Histology       good  \n",
      "1741\n"
     ]
    }
   ],
   "source": [
    "ontolist = ['bao','chmo','edamt','efo','mmo','ncit','obi']\n",
    "\n",
    "basedf = pd.DataFrame(columns=['Class ID','Original term','Term','T2T best score','TermID','Ontology','found_term','match_eval'])\n",
    "\n",
    "for eachonto in ontolist:\n",
    "    tmp = pd.read_excel(os.path.join(file_path,'measTech_T2T_test.xlsx'), eachonto, header=0,index_col=0,engine='openpyxl')\n",
    "    basedf = pd.concat((basedf,tmp),ignore_index=True)\n",
    "    \n",
    "print(basedf.head(n=2))\n",
    "\n",
    "exact_matches = pd.read_csv(os.path.join(result_path,'T2T_exact_matches.tsv'),delimiter='\\t',header=0)\n",
    "exact_term_matches = pd.read_csv(os.path.join(result_path,'T2T_term_matches.tsv'),delimiter='\\t',header=0)\n",
    "\n",
    "exact_matches['match_eval'] = 'good'\n",
    "exact_term_matches['match_eval'] = 'good'\n",
    "\n",
    "basedf = pd.concat((basedf,exact_matches,exact_term_matches),ignore_index=True)\n",
    "print(len(basedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cd2ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1741 1741\n"
     ]
    }
   ],
   "source": [
    "cleandf = basedf.drop_duplicates(keep='first')\n",
    "print(len(basedf),len(cleandf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbf526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ontology match_eval  avg T2T score  median T2T score  counts  \\\n",
      "0       bao       good       0.862667          0.862667     138   \n",
      "1       bao         ok       0.723222          0.723222       9   \n",
      "2       bao       poor       0.710385          0.710385      26   \n",
      "3      chmo       good       0.864873          0.864873     205   \n",
      "4      chmo         ok       0.905800          0.905800       5   \n",
      "5      chmo       poor       0.633214          0.633214      28   \n",
      "6      edam       good       0.898435          0.898435      23   \n",
      "7      edam         ok       0.853417          0.853417      24   \n",
      "8       efo       good       0.888406          0.888406      32   \n",
      "9       efo         ok       0.673000          0.673000       1   \n",
      "10      efo       poor       0.900000          0.900000       5   \n",
      "11      mmo       good       0.867343          0.867343     175   \n",
      "12      mmo       poor       0.816150          0.816150      20   \n",
      "13     ncit       good       0.881945          0.881945     218   \n",
      "14     ncit    invalid       0.882333          0.882333     384   \n",
      "15     ncit         ok       0.823839          0.823839      31   \n",
      "16     ncit       poor       0.732083          0.732083     361   \n",
      "17      obi       good       0.844867          0.844867      45   \n",
      "18      obi         ok       0.790000          0.790000       3   \n",
      "19      obi       poor       0.675500          0.675500       8   \n",
      "\n",
      "    lowest T2T score   std_dev  \n",
      "0              0.587  0.103534  \n",
      "1              0.627  0.074311  \n",
      "2              0.472  0.110571  \n",
      "3              0.593  0.097789  \n",
      "4              0.751  0.101976  \n",
      "5              0.507  0.105226  \n",
      "6              0.744  0.088078  \n",
      "7              0.701  0.090006  \n",
      "8              0.713  0.084699  \n",
      "9              0.673       NaN  \n",
      "10             0.816  0.091266  \n",
      "11             0.593  0.093982  \n",
      "12             0.702  0.053357  \n",
      "13             0.603  0.099992  \n",
      "14             0.876  0.004503  \n",
      "15             0.584  0.111350  \n",
      "16             0.336  0.109849  \n",
      "17             0.681  0.091270  \n",
      "18             0.728  0.078307  \n",
      "19             0.546  0.135828  \n"
     ]
    }
   ],
   "source": [
    "meanevaldf = cleandf.groupby(['Ontology','match_eval'])['T2T best score'].mean().reset_index(name='avg T2T score')\n",
    "medevaldf = cleandf.groupby(['Ontology','match_eval'])['T2T best score'].mean().reset_index(name='median T2T score')\n",
    "countevaldf = cleandf.groupby(['Ontology','match_eval']).size().reset_index(name='counts')\n",
    "mineval = cleandf.groupby(['Ontology','match_eval'])['T2T best score'].min().reset_index(name='lowest T2T score')\n",
    "stdeveval = cleandf.groupby(['Ontology','match_eval'])['T2T best score'].std().reset_index(name='std_dev')\n",
    "totalevaldf = meanevaldf.merge(medevaldf.merge(countevaldf.merge(mineval.merge(stdeveval,on=['Ontology','match_eval'],how='outer'),on=['Ontology','match_eval'],how='outer'),on=['Ontology','match_eval'],how='outer'),on=['Ontology','match_eval'],how='outer')\n",
    "print(totalevaldf)\n",
    "\n",
    "totalevaldf.to_csv(os.path.join(result_path,'T2T_results_evaluated.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e877dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
