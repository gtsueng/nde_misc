{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d852f9cd",
   "metadata": {},
   "source": [
    "## GPT measurementTechnique extraction Evaluation\n",
    "\n",
    "\n",
    "### Analyze the extraction of measurementTechniques by ChatGPT\n",
    "Approach:\n",
    "- Select 10 records at random from each of the sampled repositories\n",
    "- For each record, evaluate the techniques pulled by ChatGPT as True or False positive based on description\n",
    "- Calculate precision and recall for each record\n",
    "- If there are terms which are the same, but appear different, pull those out for standardization analysis\n",
    "\n",
    "### Analyse the extraction of extraneous generic 'stop word' terms\n",
    "Approach:\n",
    "- Split all ChatGPT generated terms into words (split by space)\n",
    "- Generate frequency table of terms\n",
    "- Identify top set of generic terms for use as permutations in standardization\n",
    "\n",
    "### Determine how well ChatGPT extractions match with measTech terms from repos\n",
    "NCBI GEO, LINCs, and REFRAMEDB have vocabulary-based (GEO), or semi-vocabulary-based (LINCs, REFRAMEDB) measurementTechnique values for their records\n",
    "Approach:\n",
    "- Run ChatGPT against 10-25 records from GEO, LINCS, and REFRAMEDB\n",
    "- Evaluate how well ChatGPT does in terms of getting a match (whether or not it can get at least 1 true positive per record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46b6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d71335",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584419f",
   "metadata": {},
   "source": [
    "### Analyze ChatGPT's extraction\n",
    "\n",
    "Sample and format the ChatGPT extracted measurementTechniques data for manual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b359a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       _id Data Repository  \\\n",
      "0  clinepidb_DS_010e5612b8       ClinEpiDB   \n",
      "1  clinepidb_DS_515a92c711       ClinEpiDB   \n",
      "\n",
      "                                           Name  \\\n",
      "0  WASH Benefits Kenya Cluster Randomized Trial   \n",
      "1               LAKANA Cluster Randomized Trial   \n",
      "\n",
      "                                         Description          Model  \\\n",
      "0  The WASH Benefits Study        Publications fr...  gpt-3.5-turbo   \n",
      "1  Background: Mass drug administration (MDA) of ...  gpt-3.5-turbo   \n",
      "\n",
      "                                         Predictions  \n",
      "0  - Cluster-randomized controlled trial - Survey...  \n",
      "1  - Mass drug administration - Cluster randomize...  \n"
     ]
    }
   ],
   "source": [
    "## Load the file\n",
    "gpt_results = pd.read_csv(os.path.join(data_path,'GPT Measurement Techniques results.tsv'),delimiter='\\t',header=0)\n",
    "print(gpt_results.head(n=2))\n",
    "\n",
    "## select 10 random records\n",
    "ransamps = gpt_results.groupby('Data Repository').sample(10)\n",
    "\n",
    "def ifenumed(rowdata):\n",
    "    numlist = ['1.','2.','3.','4.','5.','6.','7.','8.','9.','10.','11.','12.','13.','14.','15.']\n",
    "    for eachnum in numlist:\n",
    "        if eachnum in rowdata:\n",
    "            rowdata.replace(eachnum,\" - \")\n",
    "        else:\n",
    "            break\n",
    "    return rowdata\n",
    "\n",
    "## format results from records\n",
    "def clean_predictions(row):\n",
    "    rowdata = row['Predictions']\n",
    "    if '1. ' in rowdata:\n",
    "        rowdata = ifenumed(rowdata)\n",
    "    tmpdata = rowdata.replace(\" - \",\"|\")\n",
    "    tmplist = tmpdata.split(\"|\")\n",
    "    cleanlist = [x.replace(\"- \",\"\").strip() for x in tmplist]\n",
    "    return cleanlist \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "ransamps['clean_pred'] = ransamps.apply(lambda row: clean_predictions(row), axis=1)\n",
    "exploded_df = ransamps.explode('clean_pred')\n",
    "#print(ransamps.head(n=2))\n",
    "exploded_df.drop(columns=['Model','Predictions'],inplace=True)\n",
    "print(exploded_df.head(n=2))\n",
    "\n",
    "## Export results for manual evaluation\n",
    "exploded_df.to_csv(os.path.join(result_path,'GPT_sample.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585608bc",
   "metadata": {},
   "source": [
    "### Analyse the extraction of extraneous generic 'stop word' terms\n",
    "\n",
    "These generic terms can be used to mutate measurementTechnique terms for testing how the measurementTechnique mapping pipeline is affected by ChatGPT's tendency to add these types of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea804e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          technique  counts\n",
      "28         analysis      72\n",
      "172            data      44\n",
      "657      sequencing      29\n",
      "133      collection      28\n",
      "29              and      24\n",
      "711           study      21\n",
      "517              of      19\n",
      "82            blood      13\n",
      "738         testing      11\n",
      "470      microscopy      10\n",
      "646        sampling       9\n",
      "539             pcr       9\n",
      "594   questionnaire       9\n",
      "49       assessment       9\n",
      "98             cell       8\n",
      "609        receptor       8\n",
      "189      diagnostic       8\n",
      "107           chain       8\n",
      "739           tests       8\n",
      "234         ethical       7\n",
      "248      expression       7\n",
      "147         consent       7\n",
      "183          design       7\n",
      "633          review       7\n",
      "200             dna       7\n",
      "345  identification       7\n",
      "606        reaction       6\n",
      "387      interviews       6\n",
      "715    surveillance       6\n",
      "434         mapping       6\n",
      "395       isolation       6\n",
      "276             for       6\n",
      "300      geographic       6\n",
      "185       detection       6\n",
      "559      polymerase       6\n",
      "220           elisa       6\n",
      "587         protein       6\n",
      "782           using       6\n"
     ]
    }
   ],
   "source": [
    "## Pull the terms from Dylan's list (since the source is not needed)\n",
    "t2t_results = pd.read_csv(os.path.join(data_path,'Measurement Techniques mapped.tsv'),delimiter='\\t',header=2)\n",
    "\n",
    "## Pull the list of techniques\n",
    "techniques = t2t_results['Technique'].tolist()\n",
    "\n",
    "## process the terms\n",
    "cleanlist = []\n",
    "for eachterm in techniques:\n",
    "    tmpterm = eachterm.lower()\n",
    "    tmpterm.replace(\"-\",\" \").replace(\":\",\" \")\n",
    "    tmplist = tmpterm.split(\" \")\n",
    "    cleantmp = [x.replace(\"(\",\"\").replace(\")\",\"\").strip() for x in tmplist]\n",
    "    cleanlist.extend(cleantmp)\n",
    "\n",
    "termseries = pd.Series(cleanlist)\n",
    "termdf = termseries.to_frame('technique')\n",
    "termfreq = termdf.groupby('technique').size().reset_index(name='counts')\n",
    "termfreq.sort_values('counts',ascending=False,inplace=True)\n",
    "print(termfreq.loc[termfreq['counts']>5])\n",
    "termfreq.to_csv(os.path.join(result_path,'stopword_freq.tsv'),sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76abb2a",
   "metadata": {},
   "source": [
    "### Determine how well ChatGPT extractions match with measTech terms from repos\n",
    "\n",
    "#### Format the ChatGPT predictions for ReFRAMEDB, GEO, and LINCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c817fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(row):\n",
    "    rowdata = row['Predictions']\n",
    "    if '1. ' in rowdata:\n",
    "        rowdata = ifenumed(rowdata)\n",
    "    cleanlist = list(map(str.strip, rowdata.strip(\"][\").replace(\"'\", \"\").split(\",\")))\n",
    "    return cleanlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c0126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _id                                               Name  \\\n",
      "0   GSE57323  microRNA Expression Profile on Stimulated Peri...   \n",
      "1  GSE108631  Genome-wide maps of EWS-FLI1 binding sites and...   \n",
      "\n",
      "                                         Description          Model  \\\n",
      "0  Background: The emerging relationship between ...  gpt-3.5-turbo   \n",
      "1  We identified global DNA binding properties of...  gpt-3.5-turbo   \n",
      "\n",
      "                                         Predictions  \n",
      "0  ['RNA extraction', 'miRNA profiling using TaqM...  \n",
      "1                    ['ChIP-seq', 'Hi-C', 'RNA-Seq']  \n"
     ]
    }
   ],
   "source": [
    "raw_25 = pd.read_csv(os.path.join(data_path,'GPT_GEO_ReframeDB_measTech_results.tsv'),delimiter='\\t',header=0)\n",
    "print(raw_25.head(n=2))\n",
    "\n",
    "raw_10 = pd.read_csv(os.path.join(data_path,'GPT_LINCS_NCBI GEO_ReframeDB.tsv'),delimiter='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef42ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        _id                                         clean_pred\n",
      "0  GSE57323                                     RNA extraction\n",
      "0  GSE57323  miRNA profiling using TaqMan® Array Human micr...\n"
     ]
    }
   ],
   "source": [
    "raw_25['clean_pred'] = raw_25.apply(lambda row: format_predictions(row), axis=1)\n",
    "exploded_df = raw_25.explode('clean_pred')\n",
    "clean_25 = exploded_df.drop(['Name','Description','Model','Predictions'],axis=1)\n",
    "print(clean_25.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b693abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        _id                 clean_pred\n",
      "0  lds-1013  Competition binding assay\n",
      "0  lds-1013           KinomeScan assay\n",
      "937\n"
     ]
    }
   ],
   "source": [
    "raw_10['clean_pred'] = raw_10.apply(lambda row: format_predictions(row), axis=1)\n",
    "exploded_10 = raw_10.explode('clean_pred')\n",
    "clean_10 = exploded_10.drop(['Model','Predictions','name','description','Measurement Technique'],axis=1)\n",
    "print(clean_10.head(n=2))\n",
    "\n",
    "clean_df = pd.concat((clean_25,clean_10),ignore_index=True)\n",
    "clean_df['_id']=clean_df['_id'].astype(str).str.lower()\n",
    "clean_df.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766e9662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        _id                                         clean_pred\n",
      "0  gse57323                                     RNA extraction\n",
      "1  gse57323  miRNA profiling using TaqMan® Array Human micr...\n"
     ]
    }
   ],
   "source": [
    "print(clean_df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31cabe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "clean_id_list = clean_df['_id'].unique().tolist()\n",
    "print(len(clean_id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe78af",
   "metadata": {},
   "source": [
    "#### Pull the measurementTechnique values for the IDs of the records that were extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f508312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_measTech(jsonresult):\n",
    "    tmpTechlist = []\n",
    "    for eachhit in jsonresult['hits']:\n",
    "        nde_id = eachhit['_id']\n",
    "        if isinstance(eachhit['measurementTechnique'],list):\n",
    "            for eachmeas in eachhit['measurementTechnique']:\n",
    "                tmpdict = {\"_id\":nde_id, 'measurementTechnique':eachmeas['name']}\n",
    "                tmpTechlist.append(tmpdict)\n",
    "        elif isinstance(eachhit['measurementTechnique'],dict):\n",
    "            tmpTechlist.append({\"_id\":nde_id, 'measurementTechnique':eachhit['measurementTechnique']['name']})\n",
    "    return tmpTechlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722d2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measTechs(clean_id_list):\n",
    "    measTechlist = []\n",
    "    for nde_id in clean_id_list:\n",
    "        api_url = f'https://api-staging.data.niaid.nih.gov/v1/query?&q=identifier%3A\"{nde_id}\"&fields=measurementTechnique'\n",
    "        r = requests.get(api_url)\n",
    "        result = json.loads(r.text)\n",
    "        if len(result['hits'])>0:\n",
    "            tmplist = parse_measTech(result)\n",
    "            measTechlist.extend(tmplist)\n",
    "        else:\n",
    "            api_url = f'https://api-staging.data.niaid.nih.gov/v1/query?&q={nde_id}&fields=measurementTechnique'\n",
    "            r = requests.get(api_url)\n",
    "            result = json.loads(r.text)\n",
    "            tmplist = parse_measTech(result)\n",
    "            measTechlist.extend(tmplist)\n",
    "    measTechdf = pd.DataFrame(measTechlist)    \n",
    "    return measTechdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a97c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _id                               measurementTechnique\n",
      "0   gse57323                     Expression profiling by RT-PCR\n",
      "1  gse108631  Genome binding/occupancy profiling by high thr...\n",
      "218\n",
      "CPU times: total: 17 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "measTechdf = get_measTechs(clean_id_list)\n",
    "print(measTechdf.head(n=2))\n",
    "print(len(measTechdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b026253",
   "metadata": {},
   "source": [
    "#### Evaluate if ChatGPT successfully extracted the measurementTechnique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc6735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _id                               measurementTechnique\n",
      "0   gse57323                     Expression profiling by RT-PCR\n",
      "1  gse108631  Genome binding/occupancy profiling by high thr...\n"
     ]
    }
   ],
   "source": [
    "measTechdf['_id'] = measTechdf['_id'].astype(str).str.lower()\n",
    "print(measTechdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92287db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937 1226\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "merged_result = clean_df.merge(measTechdf, on='_id', how='left')\n",
    "merged_result.drop_duplicates(keep='first')\n",
    "print(len(clean_df),len(merged_result))\n",
    "print(len(merged_result['_id'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85b73a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        _id                                         clean_pred  \\\n",
      "0  gse57323                                     RNA extraction   \n",
      "1  gse57323  miRNA profiling using TaqMan® Array Human micr...   \n",
      "\n",
      "             measurementTechnique  \n",
      "0  Expression profiling by RT-PCR  \n",
      "1  Expression profiling by RT-PCR  \n"
     ]
    }
   ],
   "source": [
    "print(merged_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c705ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### Basic comparisons\n",
    "\n",
    "## Try matching by exact match\n",
    "tmp4test = merged_result.copy()\n",
    "tmp4test['clean_pred'] = tmp4test['clean_pred'].astype(str).str.lower()\n",
    "tmp4test['measurementTechnique'] = tmp4test['measurementTechnique'].astype(str).str.lower()\n",
    "tmp4test['match_text?'] = tmp4test['clean_pred'].equals(tmp4test['measurementTechnique'])\n",
    "matched = tmp4test.loc[tmp4test['match_text?']==True]\n",
    "print(len(matched))\n",
    "\n",
    "## Try matching by length\n",
    "tmp4test = merged_result.copy()\n",
    "tmp4test['clean_len'] = [len(x) for x in tmp4test['clean_pred']]\n",
    "tmp4test['meas_len'] = tmp4test['measurementTechnique'].astype(str).str.len()\n",
    "tmp4test['match_len?'] = tmp4test['clean_len'].equals(tmp4test['meas_len'])\n",
    "matched = tmp4test.loc[tmp4test['match_len?']==True]\n",
    "print(len(matched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d243b9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        _id                                         clean_pred  \\\n",
      "0  gse57323                                     RNA extraction   \n",
      "1  gse57323  miRNA profiling using TaqMan® Array Human micr...   \n",
      "\n",
      "             measurementTechnique      jsim  \n",
      "0  Expression profiling by RT-PCR  0.000000  \n",
      "1  Expression profiling by RT-PCR  0.083333  \n"
     ]
    }
   ],
   "source": [
    "### Advanced comparisons\n",
    "\n",
    "## Try calculating Jaccard similarity\n",
    "def get_jsim(row):\n",
    "    tmpbag1 = str(row['clean_pred']).split(' ')\n",
    "    tmpbag2 = str(row['measurementTechnique']).split(' ')\n",
    "    bag1 = set([x.lower() for x in tmpbag1])\n",
    "    bag2 = set([x.lower() for x in tmpbag2])\n",
    "    bag_intersect = bag1.intersection(bag2)\n",
    "    bag_union = bag1.union(bag2)\n",
    "    jsim = len(bag_intersect)/len(bag_union)\n",
    "    return jsim\n",
    "\n",
    "merged_result['jsim'] = merged_result.apply(lambda row: get_jsim(row),axis=1)\n",
    "print(merged_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9b1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reduce the number of manual evaluations needed\n",
    "sorted_merged = merged_result.sort_values(by='jsim',ascending = False)\n",
    "best_matches = sorted_merged.drop_duplicates(subset='_id',keep='first')\n",
    "\n",
    "### Export the best_matches for evaluation\n",
    "best_matches.to_csv(os.path.join(result_path,'Lincs_GEO_reframe_best_for_evaluation.tsv'),sep='\\t',header=True)\n",
    "\n",
    "### Export the all_matches for evaluation\n",
    "sorted_merged.to_csv(os.path.join(result_path,'Lincs_GEO_reframe_for_evaluation.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fb291",
   "metadata": {},
   "source": [
    "### Manually evaluate the results\n",
    "Since GPT can potentially identify more techniques than are available as vocabulary-controlled categories in GEO, ReFRAMEDB, LINCS, we are looking only for the number of matches relative to the total number of records tested. The number of \"false\" predictions (ie- predictions that don't match the measTech values) are not being evaluated here since it is possible that they are \"true\" predictions with no matches due to limitations in the vocabulary used for the repository "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ab8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef169b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "data_path = os.path.join(script_path,'data')\n",
    "result_path = os.path.join(script_path,'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fc4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      _id                                    clean_pred  \\\n",
      "947  gse1  Clustering based on correlation coefficients   \n",
      "946  gse1                     Gene expression profiling   \n",
      "\n",
      "              measurementTechnique  jsim      evaluation  \n",
      "947  Expression profiling by array   0.0            poor  \n",
      "946  Expression profiling by array   0.4  GPT more broad  \n"
     ]
    }
   ],
   "source": [
    "evaluateddf = pd.read_csv(os.path.join(result_path,'Lincs_GEO_reframe_evaluated.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(evaluateddf.head(n=2))\n",
    "all_evaluated_ids = evaluateddf['_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc745f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_found = evaluateddf.loc[evaluateddf['evaluation']!='poor']\n",
    "unmatched = evaluateddf.loc[evaluateddf['evaluation']=='poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d02068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "unique_records_measTech_combos = evaluateddf.groupby(['_id','measurementTechnique']).size().reset_index(name='counts')\n",
    "print(len(unique_records_measTech_combos))\n",
    "resultdict = {\"Total number of records evaluated\": len(all_evaluated_ids),\n",
    "              \"Total number of unique records & measurementTechnique combos evaluated\": len(unique_records_measTech_combos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44624fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 146 25 171\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "good_match = evaluateddf.loc[evaluateddf['evaluation']=='good']\n",
    "ok_match = evaluateddf.loc[(evaluateddf['evaluation']=='GPT more specific')|(evaluateddf['evaluation']=='GPT more broad')]\n",
    "good_match_ids = good_match['_id'].unique().tolist()\n",
    "ok_match_ids = ok_match['_id'].unique().tolist()\n",
    "all_matched_ids = list(set(good_match_ids).union(set(ok_match_ids)))\n",
    "match_found_ids = match_found['_id'].unique().tolist()\n",
    "true_unmatched = [x for x in all_evaluated_ids if x not in match_found_ids]\n",
    "missing_matched_ids = [x for x in match_found_ids if x not in all_matched_ids]\n",
    "print(len(all_matched_ids),len(match_found_ids), len(true_unmatched),len(all_evaluated_ids)) \n",
    "accounted_ids = list(set(all_matched_ids).union(set(true_unmatched)))\n",
    "unaccounted_ids = [x for x in all_evaluated_ids if x not in accounted_ids]\n",
    "print(len(accounted_ids))\n",
    "\n",
    "resultdict[\"Number of records with a good match\"] = len(good_match_ids)\n",
    "resultdict[\"Number of records with an ok match (slightly more broad or specific)\"] = len(ok_match_ids)\n",
    "resultdict[\"Number of records with ok or better match\"] = len(all_matched_ids)\n",
    "resultdict[\"Number of records where GPT did not identify at least one matching measurementTechnique\"] = len(true_unmatched)\n",
    "resultdict[\"Ratio total found\"] = len(all_matched_ids)/len(all_evaluated_ids)\n",
    "resultdict[\"good match ratio\"] = len(good_match_ids)/len(all_evaluated_ids)\n",
    "resultdict[\"ok match ratio\"] = len(ok_match_ids)/len(all_evaluated_ids)\n",
    "resultdict[\"unmatched ration\"] = len(true_unmatched)/len(all_evaluated_ids)\n",
    "\n",
    "\n",
    "with open(os.path.join(result_path,'GEO_LINCS_Reframe_analysis.txt'),'w') as outwrite:\n",
    "    for k in list(resultdict.keys()):\n",
    "        outwrite.write(k+'\\t'+str(resultdict[k])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c27a35",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3694eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 323, 'total': 1, 'max_score': 24.680622, 'hits': [{'_id': 'reframedb_a00414', '_ignored': ['all.keyword'], '_score': 24.680622, 'measurementTechnique': [{'name': 'cell based'}]}]}\n"
     ]
    }
   ],
   "source": [
    "### Test the parse_measTech function\n",
    "\n",
    "nde_id = 'reframedb_a00414'\n",
    "api_url = f'https://api-staging.data.niaid.nih.gov/v1/query?&q=identifier%3A\"{nde_id}\"&fields=measurementTechnique'\n",
    "r = requests.get(api_url)\n",
    "result = json.loads(r.text)\n",
    "print(result)\n",
    "tmpTechlist = parse_measTech(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67c765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
