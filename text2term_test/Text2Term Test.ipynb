{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79a03664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2term\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b495586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-11 08:07:54 INFO [text2term.term_collector]: Loading ontology https://purl.obolibrary.org/obo/ncbitaxon.owl...\n",
      "2023-11-11 08:21:48 INFO [text2term.term_collector]: ...done (ontology loading time: 589.27s)\n",
      "2023-11-11 08:21:48 INFO [text2term.term_collector]: Collecting ontology term details...\n",
      "2023-11-11 08:28:29 INFO [text2term.term_collector]: ...done: collected 2523789 ontology terms (collection time: 401.07s)\n",
      "2023-11-11 08:33:36 INFO [text2term.t2t]: Filtered ontology terms to those of type: any\n",
      "2023-11-11 08:33:37 INFO [text2term.t2t]: Caching ontology https://purl.obolibrary.org/obo/ncbitaxon.owl to: cache\\ncbitaxon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<text2term.onto_cache.OntologyCache at 0x275720ebfd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2term.cache_ontology(\"https://purl.obolibrary.org/obo/ncbitaxon.owl\", \"ncbitaxon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ed2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                                species  \n",
      "0  [{'name': 'Caenorhabditis elegans'}]  \n",
      "1  [{'name': 'Caenorhabditis elegans'}]  \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data','raw_species_results.pickle'),'rb') as infile:\n",
    "    raw_species = pickle.load(infile)\n",
    "\n",
    "print(raw_species.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26afc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200837\n",
      "202257\n",
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                              species  \n",
      "0  {'name': 'Caenorhabditis elegans'}  \n",
      "1  {'name': 'Caenorhabditis elegans'}  \n"
     ]
    }
   ],
   "source": [
    "print(len(raw_species))\n",
    "\n",
    "raw_boom = raw_species.explode('species')\n",
    "print(len(raw_boom))\n",
    "print(raw_boom.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3154fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                              species            species_name  flag_raised  \n",
      "0  {'name': 'Caenorhabditis elegans'}  Caenorhabditis elegans        False  \n",
      "1  {'name': 'Caenorhabditis elegans'}  Caenorhabditis elegans        False  \n"
     ]
    }
   ],
   "source": [
    "def pop_species(species_dict):\n",
    "    if isinstance(species_dict,dict):\n",
    "        x = species_dict['name']\n",
    "    elif isinstance(species_dict,str):\n",
    "        x = species_dict\n",
    "    return x\n",
    "\n",
    "## Raise a flag if the species was not formatted as a dictionary\n",
    "def flag_species(species_dict):\n",
    "    if isinstance(species_dict,dict):\n",
    "        x = False\n",
    "    elif isinstance(species_dict,str):\n",
    "        x = True\n",
    "    return x    \n",
    "\n",
    "raw_boom['species_name'] = raw_boom.apply(lambda row: pop_species(row['species']),axis=1)\n",
    "raw_boom['flag_raised'] = raw_boom.apply(lambda row: flag_species(row['species']),axis=1)\n",
    "print(raw_boom.head(n=2))\n",
    "clean_dict = raw_boom[['_id','name','species_name','flag_raised']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97115ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id                                               name  \\\n",
      "0  GEO_GSE110840  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  GEO_GSE110842  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "             species_name  flag_raised  \n",
      "0  Caenorhabditis elegans        False  \n",
      "1  Caenorhabditis elegans        False  \n"
     ]
    }
   ],
   "source": [
    "print(clean_dict.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae479db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2565\n"
     ]
    }
   ],
   "source": [
    "no_dups = clean_dict['species_name'].unique().tolist()\n",
    "print(len(no_dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b41a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 10:38:58 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-13 10:41:30 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-13 10:41:30 INFO [text2term.t2t]: Mapping 2565 source terms to ncbitaxon\n",
      "2023-11-13 10:46:52 INFO [text2term.t2t]: ...done (mapping time: 321.58s seconds)\n",
      "CPU times: total: 7min 57s\n",
      "Wall time: 8min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t2t_result = text2term.map_terms(no_dups, \"ncbitaxon\", use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d7b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Source Term ID    Source Term  \\\n",
      "3180  http://ccb.hms.harvard.edu/t2t/R3Go4L6RwRz  Abies pinsapo   \n",
      "3181  http://ccb.hms.harvard.edu/t2t/R3Go4L6RwRz  Abies pinsapo   \n",
      "\n",
      "               Mapped Term Label Mapped Term CURIE  \\\n",
      "3180               Abies pinsapo   NCBITAXON:56046   \n",
      "3181  Abies pinsapo var. pinsapo  NCBITAXON:928732   \n",
      "\n",
      "                                      Mapped Term IRI  Mapping Score  Tags  \n",
      "3180   http://purl.obolibrary.org/obo/NCBITaxon_56046          0.997  None  \n",
      "3181  http://purl.obolibrary.org/obo/NCBITaxon_928732          0.923  None  \n"
     ]
    }
   ],
   "source": [
    "t2t_result.sort_values(['Source Term','Mapping Score'], ascending = [True,False], inplace=True)\n",
    "print(t2t_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f529b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2564\n"
     ]
    }
   ],
   "source": [
    "top_score = t2t_result.drop_duplicates('Source Term',keep='first')\n",
    "print(len(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a08e5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_dictdf = top_score[['Source Term','Mapped Term Label','Mapped Term CURIE', 'Mapping Score']].copy()\n",
    "t2t_dictdf.rename(columns={'Source Term':'species_name', 'Mapped Term CURIE':'CURIE','Mapped Term Label':'Label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b25ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202256\n",
      "202182\n"
     ]
    }
   ],
   "source": [
    "mapped_dict = clean_dict.merge(t2t_dictdf, on='species_name', how='inner')\n",
    "print(len(mapped_dict))\n",
    "mapped_dict.drop_duplicates(keep='first', inplace=True)\n",
    "print(len(mapped_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "610b5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197509\n"
     ]
    }
   ],
   "source": [
    "no_flags = mapped_dict.loc[mapped_dict['flag_raised']==False]\n",
    "flagged = mapped_dict.loc[mapped_dict['flag_raised']==True]\n",
    "print(len(no_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e173a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_flags.to_csv(os.path.join('data','t2t_mapped_no_flags.tsv'), sep='\\t', header=True)\n",
    "flagged.to_csv(os.path.join('data','t2t_mapped_flagged.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e948d8",
   "metadata": {},
   "source": [
    "## Process the data from the EXTRACT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "996687e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set filepaths\n",
    "script_path = os.getcwd()\n",
    "parent_path = os.path.dirname(script_path)\n",
    "input_path = os.path.join(parent_path,'EXTRACT_check','data')\n",
    "input_file = os.path.join(input_path,'test_100000.tsv')\n",
    "output_path = os.path.join(script_path,'data')\n",
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db314231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210242\n",
      "         _id extracted_term  extracted_type  taxid           CURIE\n",
      "0  GSE238109         bovine              -2   9913  NCBITAXON:9913\n",
      "1  GSE187829          human              -2   9606  NCBITAXON:9606\n"
     ]
    }
   ],
   "source": [
    "extract_results = pd.read_csv(input_file,delimiter='\\t',header=0,index_col=0)\n",
    "print(len(extract_results))\n",
    "print(extract_results.head(n=2))\n",
    "stats['number of results from EXTRACT']=len(extract_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4702fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(extract_results.loc[(extract_results['extracted_type']!=-2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4576d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8851\n",
      "['bovine', 'human', 'mouse', 'mice', 'murine', 'Human', 'P. gingivalis', 'Porphyromonas gingivalis', 'Mouse', 'MCMV']\n"
     ]
    }
   ],
   "source": [
    "unique_terms = extract_results['extracted_term'].unique().tolist()\n",
    "print(len(unique_terms))\n",
    "print(unique_terms[0:10])\n",
    "stats['number of unique extracted terms']=len(unique_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5253a5",
   "metadata": {},
   "source": [
    "### Check if casing matters in processing of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1ae19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7727\n",
      "['artemisia afra', 'p. yoelii nigeriensis', 'pyrus pyrifolia', 'lined ground squirrels', 'anabaena pcc7120', 'potato psyllid', 'caenorhabditis elegans', 'anopheles moucheti', 'b. germanica', 'b. bacteriovorus']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type datetime is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent change if casing adjusted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(unique_terms)\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(lower_unique)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_terms))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2t_EXTRACT_test.json\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outwrite:\n\u001b[1;32m---> 12\u001b[0m     outwrite\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type datetime is not JSON serializable"
     ]
    }
   ],
   "source": [
    "## Note capitalization introduces duplication; however, proper casing can be useful in species determination\n",
    "## We'll also test whether or not capitalization affects Text2Term mappings\n",
    "lower_unique = list(set([x.lower() for x in unique_terms]))\n",
    "print(len(lower_unique))\n",
    "print(lower_unique[0:10])\n",
    "\n",
    "## The lower casing and de-duplication removed only about 12% of the terms\n",
    "stats['number of unique terms if casing adjusted (to lower case)']= len(lower_unique)\n",
    "stats['percent change if casing adjusted'] = (len(unique_terms)-len(lower_unique)/len(unique_terms))\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d489ddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.016336\n",
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418}\n"
     ]
    }
   ],
   "source": [
    "uniquestarttime = datetime.now()\n",
    "time.sleep(1)\n",
    "uniqueendtime = datetime.now()\n",
    "duration = uniqueendtime-uniquestarttime\n",
    "print(duration)\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c62b58a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418}\n",
      "2023-11-18 08:49:02 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 08:49:44 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 08:49:44 INFO [text2term.t2t]: Mapping 8851 source terms to ncbitaxon\n",
      "2023-11-18 08:54:48 INFO [text2term.t2t]: ...done (mapping time: 303.78s seconds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:16\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_type' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)\n",
    "uniquestarttime = datetime.now()\n",
    "stats['Time start for T2T test on unique terms (casing not adjusted)'] = datetime.strftime(uniquestarttime,'%m/%d/%Y, %H:%M:%S')\n",
    "\n",
    "unique_t2t_result = text2term.map_terms(unique_terms, \"ncbitaxon\", use_cache=True)\n",
    "\n",
    "uniqueendtime = datetime.now()\n",
    "stats['Time end for T2T test on unique terms (casing not adjusted)'] = datetime.strftime(uniqueendtime,'%m/%d/%Y, %H:%M:%S')\n",
    "stats['Duration of T2T test on unique terms (casing not adjusted)'] = str(uniqueendtime - uniquestarttime)\n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_unique_t2t_test.pickle'),'wb') as outfile:\n",
    "    pickle.dump(unique_t2t_result,outfile)\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf48fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418, 'Time start for T2T test on unique terms (casing not adjusted)': '11/18/2023, 08:49:02', 'Time end for T2T test on unique terms (casing not adjusted)': '11/18/2023, 08:57:08', 'Duration of T2T test on unique terms (casing not adjusted)': '0:08:06.170061'}\n",
      "2023-11-18 10:16:59 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 10:17:54 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 10:17:54 INFO [text2term.t2t]: Mapping 7727 source terms to ncbitaxon\n",
      "2023-11-18 10:23:27 INFO [text2term.t2t]: ...done (mapping time: 332.62s seconds)\n",
      "CPU times: total: 8min 34s\n",
      "Wall time: 9min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)\n",
    "\n",
    "lowerstarttime = datetime.now()\n",
    "stats['Time start for T2T test on unique terms (casing adjusted)'] = datetime.strftime(lowerstarttime,'%m/%d/%Y, %H:%M:%S')\n",
    "\n",
    "lower_t2t_result = text2term.map_terms(lower_unique, \"ncbitaxon\", use_cache=True)\n",
    "\n",
    "lowerendtime = datetime.now()\n",
    "stats['Time end for T2T test on unique terms (casing adjusted)'] = datetime.strftime(lowerendtime,'%m/%d/%Y, %H:%M:%S')\n",
    "stats['Duration of T2T test on unique terms (casing adjusted)'] = str(lowerendtime - lowerstarttime)\n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_lower_t2t_test.pickle'),'wb') as loweroutfile:\n",
    "    pickle.dump(lower_t2t_result,loweroutfile)  \n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afee58",
   "metadata": {},
   "source": [
    "#### Actually run the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2be2c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Source Term ID original mapped term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/RKMJbF6HJke               bovine   \n",
      "1  http://ccb.hms.harvard.edu/t2t/RKMJbF6HJke               bovine   \n",
      "\n",
      "  Mapped Term Label  Mapped Term CURIE  \\\n",
      "0        Bos taurus     NCBITAXON:9913   \n",
      "1  Bovine nebovirus  NCBITAXON:1661258   \n",
      "\n",
      "                                    Mapped Term IRI  Original Mapping Score  \\\n",
      "0     http://purl.obolibrary.org/obo/NCBITaxon_9913                   0.986   \n",
      "1  http://purl.obolibrary.org/obo/NCBITaxon_1661258                   0.772   \n",
      "\n",
      "   Tags Source Term  \n",
      "0  None      bovine  \n",
      "1  None      bovine  \n",
      "                               Source Term ID     Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/RKFFMZsif39  artemisia afra   \n",
      "1  http://ccb.hms.harvard.edu/t2t/RKFFMZsif39  artemisia afra   \n",
      "\n",
      "  Mapped Term Label Mapped Term CURIE  \\\n",
      "0    Artemisia afra   NCBITAXON:72333   \n",
      "1         Artemisia    NCBITAXON:4219   \n",
      "\n",
      "                                  Mapped Term IRI  Lower Case Map Score  Tags  \n",
      "0  http://purl.obolibrary.org/obo/NCBITaxon_72333                 0.997  None  \n",
      "1   http://purl.obolibrary.org/obo/NCBITaxon_4219                 0.823  None  \n"
     ]
    }
   ],
   "source": [
    "## Compare processing of case-adjusted vs original case mappings\n",
    "with open(os.path.join(output_path,'EXTRACT_lower_t2t_test.pickle'),'rb') as loweroutfile:\n",
    "    lower_t2t_result = pickle.load(loweroutfile)  \n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_unique_t2t_test.pickle'),'rb') as outfile:\n",
    "    unique_t2t_result = pickle.load(outfile) \n",
    "    \n",
    "## rename columns and add lower case column in preparation for a merge\n",
    "unique_t2t_result.rename(columns={'Source Term':'original mapped term','Mapping Score':'Original Mapping Score'}, inplace=True)\n",
    "unique_t2t_result['Source Term'] = [x.lower() for x in unique_t2t_result['original mapped term']]\n",
    "\n",
    "lower_t2t_result.rename(columns={'Mapping Score':'Lower Case Map Score'},inplace = True)\n",
    "\n",
    "print(unique_t2t_result.head(n=2))\n",
    "print(lower_t2t_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a44f65ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8844\n",
      "7720\n",
      "8678\n",
      "    original mapped term   Source Term   Mapped Term Label  Mapped Term CURIE  \\\n",
      "71            A. linaria    a. linaria     Linaria <birds>  NCBITAXON:1930290   \n",
      "108         A. stephensi  a. stephensi  Stephensia <moths>  NCBITAXON:1073692   \n",
      "134                 abaM          abam       Musa textilis   NCBITAXON:228183   \n",
      "246                Aedes         aedes    Aedes <subgenus>   NCBITAXON:149531   \n",
      "\n",
      "     Original Mapping Score  Lower Case Map Score  \n",
      "71                    0.938                   NaN  \n",
      "108                   0.823                   NaN  \n",
      "134                   0.495                   NaN  \n",
      "246                   0.999                   NaN  \n"
     ]
    }
   ],
   "source": [
    "unique2merge = unique_t2t_result[['original mapped term','Source Term','Mapped Term Label','Mapped Term CURIE','Original Mapping Score']].copy()\n",
    "unique2merge.sort_values(['Source Term','Original Mapping Score'], ascending=[True,False],inplace=True)\n",
    "unique2merge.drop_duplicates(subset=['original mapped term','Source Term'],keep='first',inplace=True)\n",
    "\n",
    "lower2merge = lower_t2t_result[['Source Term','Mapped Term Label','Mapped Term CURIE','Lower Case Map Score']].copy()\n",
    "lower2merge.sort_values(['Source Term','Lower Case Map Score'], ascending=[True,False],inplace=True)\n",
    "lower2merge.drop_duplicates(subset=['Source Term'],keep='first',inplace=True)\n",
    "print(len(unique2merge))\n",
    "print(len(lower2merge))\n",
    "\n",
    "mergeddf = unique2merge.merge(lower2merge,on=['Source Term','Mapped Term Label','Mapped Term CURIE'],how='left')\n",
    "matchedmergeddf = unique2merge.merge(lower2merge,on=['Source Term','Mapped Term Label','Mapped Term CURIE'],how='inner')\n",
    "print(len(matchedmergeddf))\n",
    "\n",
    "lower_map_fail = mergeddf.loc[mergeddf['Lower Case Map Score'].isna()]\n",
    "print(lower_map_fail.head(n=4))\n",
    "\n",
    "## inspect some of the failures to see if any improvements can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ae745aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227\n",
      "2877\n",
      "4574\n",
      "              original mapped term                      Source Term  \\\n",
      "2                            5z489                            5z489   \n",
      "3                           A- RNA                           a- rna   \n",
      "4                            A-RNA                            a-rna   \n",
      "9  A. actinomycetemcomitans HK1651  a. actinomycetemcomitans hk1651   \n",
      "\n",
      "                              Mapped Term Label  Mapped Term CURIE  \\\n",
      "2                              Enytus sp. Oz489  NCBITAXON:2071065   \n",
      "3                                 RNA virus sp.  NCBITAXON:2217650   \n",
      "4                                 RNA virus sp.  NCBITAXON:2217650   \n",
      "9  Aggregatibacter actinomycetemcomitans HK1651   NCBITAXON:272556   \n",
      "\n",
      "   Original Mapping Score  Lower Case Map Score  \n",
      "2                   0.448                 0.447  \n",
      "3                   0.756                 0.755  \n",
      "4                   0.756                 0.755  \n",
      "9                   0.816                 0.815  \n",
      "=================\n",
      "  original mapped term    Source Term  \\\n",
      "0            2019-nCoV      2019-ncov   \n",
      "1            2019_nCoV      2019_ncov   \n",
      "6        A.  fumigatus  a.  fumigatus   \n",
      "7             A. aceti       a. aceti   \n",
      "\n",
      "                                 Mapped Term Label  Mapped Term CURIE  \\\n",
      "0  Severe acute respiratory syndrome coronavirus 2  NCBITAXON:2697049   \n",
      "1  Severe acute respiratory syndrome coronavirus 2  NCBITAXON:2697049   \n",
      "6             Aspergillus fumigatus var. fumigatus    NCBITAXON:41122   \n",
      "7                                Acetobacter aceti      NCBITAXON:435   \n",
      "\n",
      "   Original Mapping Score  Lower Case Map Score  \n",
      "0                   0.598                 0.601  \n",
      "1                   0.598                 0.601  \n",
      "6                   0.844                 0.845  \n",
      "7                   0.783                 0.784  \n",
      "=================\n",
      "        original mapped term               Source Term  \\\n",
      "5                      a-TEA                     a-tea   \n",
      "8   A. actinomycetemcomitans  a. actinomycetemcomitans   \n",
      "10              A. aculeatus              a. aculeatus   \n",
      "12                A. aegypti                a. aegypti   \n",
      "\n",
      "                        Mapped Term Label  Mapped Term CURIE  \\\n",
      "5                             Tea A virus  NCBITAXON:2941489   \n",
      "8   Aggregatibacter actinomycetemcomitans      NCBITAXON:714   \n",
      "10       Gasterosteus aculeatus aculeatus   NCBITAXON:481459   \n",
      "12                  Aedes aegypti aegypti  NCBITAXON:1424507   \n",
      "\n",
      "    Original Mapping Score  Lower Case Map Score  \n",
      "5                    0.832                 0.832  \n",
      "8                    0.748                 0.748  \n",
      "10                   0.838                 0.838  \n",
      "12                   0.915                 0.915  \n"
     ]
    }
   ],
   "source": [
    "## Check if score for terms which were mapped twice are higher or lower based on casing\n",
    "original_higher = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']>matchedmergeddf['Lower Case Map Score']]\n",
    "lower_case_higher = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']<matchedmergeddf['Lower Case Map Score']]\n",
    "scores_equal = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']==matchedmergeddf['Lower Case Map Score']]\n",
    "print(len(original_higher))\n",
    "print(len(lower_case_higher))\n",
    "print(len(scores_equal))\n",
    "\n",
    "print(original_higher.head(n=4))\n",
    "print('=================')\n",
    "print(lower_case_higher.head(n=4))\n",
    "print('=================')\n",
    "print(scores_equal.head(n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119af9b",
   "metadata": {},
   "source": [
    "lower casing does not save too much time, so preserve it in order to ensure good capture of species in the [Letter. word] format\n",
    "\n",
    "In some cases, T2T will give a match to the species term a higher score even though it's in the wrong genus. To select the right match, it may be necessary to pull these cases out by regex and select the match that includes the correct genus abbreviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d75aea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A.  fumigatus', 'A. aceti', 'A. actinomycetemcomitans', 'A. actinomycetemcomitans HK1651', 'A. aculeatus', 'A. adenophora', 'A. aegypti', 'A. afra', 'A. afraspera', 'A. alternata']\n",
      "1781\n"
     ]
    }
   ],
   "source": [
    "term_w_periods = [x for x in unique2merge['original mapped term'] if '.' in x]\n",
    "print(term_w_periods[0:10])\n",
    "print(len(term_w_periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c41d416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 11:02:14 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 11:03:02 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 11:03:02 INFO [text2term.t2t]: Mapping 1781 source terms to ncbitaxon\n",
      "2023-11-18 11:06:43 INFO [text2term.t2t]: ...done (mapping time: 220.19s seconds)\n",
      "5343\n"
     ]
    }
   ],
   "source": [
    "t2t_period = text2term.map_terms(term_w_periods, \"ncbitaxon\", use_cache=True)\n",
    "print(len(t2t_period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df1300e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Source Term ID             Source Term  \\\n",
      "5333  http://ccb.hms.harvard.edu/t2t/RGgMK2pniYm              Z. mobilis   \n",
      "5334  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5335  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5336  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5337  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5338  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5339  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5340  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "5341  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "5342  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "\n",
      "                                    Mapped Term Label  Mapped Term CURIE  \\\n",
      "5333                                 Bacillus mobilis  NCBITAXON:2026190   \n",
      "5334                                       Triticinae  NCBITAXON:1648030   \n",
      "5335                              Pseudomonas tritici  NCBITAXON:2745518   \n",
      "5336                                  Anguina tritici   NCBITAXON:166006   \n",
      "5337                                         Zea mays     NCBITAXON:4577   \n",
      "5338                             Zea mays subsp. mays   NCBITAXON:381124   \n",
      "5339  Zea mays subsp. mays x Zea mays subsp. mexicana  NCBITAXON:3053866   \n",
      "5340                         Zea mays subsp. mexicana     NCBITAXON:4579   \n",
      "5341  Zea mays subsp. mays x Zea mays subsp. mexicana  NCBITAXON:3053866   \n",
      "5342                                         Zea mays     NCBITAXON:4577   \n",
      "\n",
      "                                       Mapped Term IRI  Mapping Score  Tags  \n",
      "5333  http://purl.obolibrary.org/obo/NCBITaxon_2026190          0.710  None  \n",
      "5334  http://purl.obolibrary.org/obo/NCBITaxon_1648030          0.674  None  \n",
      "5335  http://purl.obolibrary.org/obo/NCBITaxon_2745518          0.645  None  \n",
      "5336   http://purl.obolibrary.org/obo/NCBITaxon_166006          0.636  None  \n",
      "5337     http://purl.obolibrary.org/obo/NCBITaxon_4577          0.897  None  \n",
      "5338   http://purl.obolibrary.org/obo/NCBITaxon_381124          0.886  None  \n",
      "5339  http://purl.obolibrary.org/obo/NCBITaxon_3053866          0.807  None  \n",
      "5340     http://purl.obolibrary.org/obo/NCBITaxon_4579          0.845  None  \n",
      "5341  http://purl.obolibrary.org/obo/NCBITaxon_3053866          0.751  None  \n",
      "5342     http://purl.obolibrary.org/obo/NCBITaxon_4577          0.666  None  \n"
     ]
    }
   ],
   "source": [
    "t2t_period\n",
    "print(t2t_period.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d23ab",
   "metadata": {},
   "source": [
    "Otherwise, Text2Term performs quite well and the source of error is usually EXTRACT\n",
    "\n",
    "Many NER tools fail for shorter length terms simply due to increased ambiguation. For EXTRACT, determine if a threshhold number of characters should be used to limit the number of false positives introduced by EXTRACT\n",
    "\n",
    "Inspect the results of EXTRACT at three and four characters to see if a filter on the Text2Term match score can help limit the false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce47c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 12:03:46 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 12:04:36 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 12:04:36 INFO [text2term.t2t]: Mapping 41 source terms to ncbitaxon\n",
      "2023-11-18 12:07:06 INFO [text2term.t2t]: ...done (mapping time: 149.81s seconds)\n",
      "41 123\n",
      "CPU times: total: 3min 18s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "three_letter_list = ['cat','dog','owl','fox','elk','cow','pig','hen','ant','rat','eel','bat','ewe','emu','yak','sow',\n",
    "                     'CAT','Dog','OWL','Fox','Elk','Cow','Pig','Hen','Ant','Rat','Eel','Bat','Ewe','Emu','Yak','Sow',\n",
    "                     'eye','hit','man','boy','car','ass','but','why','Moo']\n",
    "three_letter_match = text2term.map_terms(three_letter_list, \"ncbitaxon\", use_cache=True)\n",
    "print(len(three_letter_list),len(three_letter_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f7a8537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Source Term ID Source Term Mapped Term Label  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/R4M5P5PZMSo         cat       Felis catus   \n",
      "1  http://ccb.hms.harvard.edu/t2t/R4M5P5PZMSo         cat    Catopuma badia   \n",
      "\n",
      "  Mapped Term CURIE                                 Mapped Term IRI  \\\n",
      "0    NCBITAXON:9685   http://purl.obolibrary.org/obo/NCBITaxon_9685   \n",
      "1   NCBITAXON:61454  http://purl.obolibrary.org/obo/NCBITaxon_61454   \n",
      "\n",
      "   Mapping Score  Tags  \n",
      "0          0.960  None  \n",
      "1          0.632  None  \n"
     ]
    }
   ],
   "source": [
    "print(three_letter_match.head(n=2))\n",
    "three_letter_match.to_csv(os.path.join(output_path,'three_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bee2b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-20 07:00:12 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 07:00:52 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 07:00:52 INFO [text2term.t2t]: Mapping 20 source terms to ncbitaxon\n",
      "2023-11-20 07:02:49 INFO [text2term.t2t]: ...done (mapping time: 116.83s seconds)\n",
      "20 60\n",
      "CPU times: total: 2min 37s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "four_letter_list = ['lion','goat','deer','bear','seal','cats','dogs','bats','Rats','Wolf','duck',\n",
    "                    'Hare','toad','Vole','Crab','clam','kids','lynx','LCMV','MRSA']\n",
    "\n",
    "four_letter_match = text2term.map_terms(four_letter_list, \"ncbitaxon\", use_cache=True)\n",
    "print(len(four_letter_list),len(four_letter_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c12ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_letter_match.to_csv(os.path.join(output_path,'four_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efd7e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "                                 Source Term ID original mapped term  \\\n",
      "6    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "7    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "8    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "108  http://ccb.hms.harvard.edu/t2t/R7Eh7VS57VF                Yeast   \n",
      "109  http://ccb.hms.harvard.edu/t2t/R7Eh7VS57VF                Yeast   \n",
      "\n",
      "             Mapped Term Label Mapped Term CURIE  \\\n",
      "6                 Mus musculus   NCBITAXON:10090   \n",
      "7                  Mus <genus>   NCBITAXON:10088   \n",
      "8             Polyplax serrata  NCBITAXON:468196   \n",
      "108  Saccharomyces pastorianus   NCBITAXON:27292   \n",
      "109         Pichia sp. Yeast 2  NCBITAXON:553839   \n",
      "\n",
      "                                     Mapped Term IRI  Original Mapping Score  \\\n",
      "6     http://purl.obolibrary.org/obo/NCBITaxon_10090                   0.999   \n",
      "7     http://purl.obolibrary.org/obo/NCBITaxon_10088                   0.999   \n",
      "8    http://purl.obolibrary.org/obo/NCBITaxon_468196                   0.852   \n",
      "108   http://purl.obolibrary.org/obo/NCBITaxon_27292                   0.793   \n",
      "109  http://purl.obolibrary.org/obo/NCBITaxon_553839                   0.758   \n",
      "\n",
      "     Tags Source Term  \n",
      "6    None       mouse  \n",
      "7    None       mouse  \n",
      "8    None       mouse  \n",
      "108  None       yeast  \n",
      "109  None       yeast  \n"
     ]
    }
   ],
   "source": [
    "five_letter_matches = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==5]\n",
    "## There are ~847 rows to review for five letter matches, reduce the number for evaluation\n",
    "five_letter_sample = random.sample(five_letter_matches['original mapped term'].unique().tolist(),33)\n",
    "five_letter_subset = five_letter_matches.loc[five_letter_matches['original mapped term'].isin(five_letter_sample)]\n",
    "\n",
    "print(len(five_letter_subset))\n",
    "print(five_letter_subset.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "10b755a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_letter_subset.to_csv(os.path.join(output_path,'five_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2534720",
   "metadata": {},
   "source": [
    "## EXTRACT character limit test\n",
    "Looking at the sampled data for five letters, it looks like Text2Term is doing a good job mapping, but the false positive rate from EXTRACT is affecting the results. Check to see the likelihood of false positives getting dropped AFTER a text2term matching due to low score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba3e755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "['rat', 'HIV', 'pig', 'p22', 'TIV', 'Rat', 'HCV', 'ape', 'dog', 'TCV', 'HeV', 'MLV', 'IAV', 'MYA', 'UCA', 'HBV', 'HPV', 'cow', 'TEV', 'IPs', 'GH1', 'e16', 'RSV', 'osa', 'mcf', 'EBV', 'CMV', 'yak', 'Bcg', 'MAV', 'P22', 'mCG', 'SSP', 'NNV', 'RRV', 'NT1', 'TBV', 'mac', 'EOS', 'rye', 'iDA', 'sIA', 'MTb', 'HEV', 'SAG', 'AKA', 'GAS', 'HOA', 'vir', 'WMV', 'chp', 'JEV', 'Eos', 'NDV', 'ULA', 'e11', 'SFs', 'MCG', 'TSV', 'Doa', 'ATV', 'DWV', 'ALV', 'bee', 'HH1', 'eos', 'AON', 'AMV', 'BLV', 'VZV', 'pax', 'YFV', 'Aon', 'VIR', 'CB1', 'SUS', 'elk', 'Bpv', 'ChP', 'DCV', 'SP5', 'RKN', 'SOa', 'RDV', 'kob', 'Cow', 'SMV', 'e17', 'SV5', 'Sp4', 'Ips', 'Oar', 'MVV', 'ami', 'e19', 'PVY', 'ula', 'PMV', 'DMV', 'pMV', 'Sus', 'AS5', 'IBV', 'SCV', 'ECC', 'ITi', 'UCa', 'DOA', 'AOA', 'cEV', 'Vir', 'HDV', 'mAC', 'mev', 'LMV', 'BDV', 'isa', 'ena', 'SpV', 'TOs', 'Hpb', 'Sag', 'DRV', 'YMW', 'HEA', 'MEV', 'MPV', 'BEV', 'NMV', 'mCF', 'IpA', 'PrV', 'PVX', 'TMV', 'sav', 'FPV', 'IVE', 'FV3', 'BOA', 'QIA', 'hH1', 'UFO', 'MvA', 'ips', 'Gh1', 'TRV', 'mTB', 'oar', 'BKV', 'UTA', 'scV', 'PoA', 'tos', 'STV', 'Yak', 'ufo', 'TAV', 'pTV', 'IFV']\n",
      "2023-11-20 08:06:26 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 08:07:06 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 08:07:06 INFO [text2term.t2t]: Mapping 159 source terms to ncbitaxon\n",
      "2023-11-20 08:08:58 INFO [text2term.t2t]: ...done (mapping time: 111.75s seconds)\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ExT2T_stat = {}\n",
    "three_letter_EXTRACT = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==3]\n",
    "ExT2T_stat['number of 3 letter terms extracted'] = len(three_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['3 letter term list'] = three_letter_EXTRACT['original mapped term'].unique().tolist()\n",
    "print(len(three_letter_EXTRACT['original mapped term'].unique().tolist()))\n",
    "print(three_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "three_letter_check = text2term.map_terms(three_letter_EXTRACT['original mapped term'].unique().tolist(), \"ncbitaxon\", use_cache=True)\n",
    "three_letter_pass = three_letter_check.loc[three_letter_check['Mapping Score']>0.95]\n",
    "ExT2T_stat['3 letter terms with >0.95 match via T2T'] = three_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 3 letter terms after T2T filter'] = len(three_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(three_letter_pass['Source Term'].unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7098d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rat', 'pig', 'Rat', 'ape', 'dog', 'MYA', 'UCA', 'cow', 'IPs', 'osa', 'yak', 'EOS', 'rye', 'sIA', 'AKA', 'HOA', 'Eos', 'ULA', 'Doa', 'bee', 'eos', 'AON', 'pax', 'Aon', 'elk', 'SOa', 'kob', 'Cow', 'Ips', 'Oar', 'ami', 'ula', 'ITi', 'UCa', 'DOA', 'AOA', 'isa', 'ena', 'HEA', 'IpA', 'IVE', 'UFO', 'ips', 'oar', 'UTA', 'PoA', 'Yak', 'ufo']\n"
     ]
    }
   ],
   "source": [
    "print(three_letter_pass['Source Term'].unique().tolist())\n",
    "## At a match score of > 0.95, the percentage of EXTRACT terms that appear to be true terms is \n",
    "## 12 / 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f5cea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "2023-11-20 08:14:23 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 08:14:54 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 08:14:54 INFO [text2term.t2t]: Mapping 370 source terms to ncbitaxon\n",
      "2023-11-20 08:16:41 INFO [text2term.t2t]: ...done (mapping time: 107.13s seconds)\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "four_letter_EXTRACT = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==4]\n",
    "ExT2T_stat['number of 4 letter terms extracted'] = len(four_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['4 letter term list'] = four_letter_EXTRACT['original mapped term'].unique().tolist()\n",
    "print(len(four_letter_EXTRACT['original mapped term'].unique().tolist()))\n",
    "four_letter_check = text2term.map_terms(four_letter_EXTRACT['original mapped term'].unique().tolist(), \"ncbitaxon\", use_cache=True)\n",
    "four_letter_pass = four_letter_check.loc[four_letter_check['Mapping Score']>0.95]\n",
    "ExT2T_stat['4 letter terms with >0.95 match via T2T'] = four_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 4 letter terms after T2T filter'] = len(four_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(four_letter_pass['Source Term'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "651895c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mice', 'Mice', 'H1N1', 'rice', 'flax', 'apes', 'rats', 'dogs', 'inti', 'corn', 'sigA', 'GAGA', 'SAGA', 'goat', 'H3N2', 'Babr', 'aRNA', 'H5N1', 'pupa', 'LIMe', 'betA', 'atlA', 'PUMA', 'MOPS', 'Glia', 'MICA', 'Rats', 'pear', 'bees', 'ARIA', 'IOBA', 'Pima', 'tasA', 'TasA', 'coma', 'aloe', 'topi', 'PilA', 'lice', 'Loma', 'H7N9', 'carp', 'oats', 'Ugni', 'moPs', 'MyLa', 'MACE', 'CreX', 'MOAs', 'Homo', 'SaGA', 'eNOS', 'duck', 'LamA', 'mASO', 'NatA', 'Clea', 'Apis', 'AtlA', 'SigA', 'RANA', 'lion', 'nusA', 'H9N2', 'Bees', 'PISA', 'naso', 'CLEA', 'Goat', 'SIMO', 'ArcA', 'ENOS', 'SigE', 'Lice', 'TAMU', 'THOR', 'MetA', 'PATU', 'RITA', 'netA', 'NetA', 'plum', 'alcA', 'Pupa', 'aroA', 'AroA', 'LAMA', 'MicA', 'nepA', 'disA', 'neem', 'rana', 'MeTA', 'sigE', 'Ucla', 'aliA', 'afer', 'rapa', 'pacu', 'hemp', 'arcA', 'pLUM', 'ANIA', 'aniA', 'deer', 'thor', 'MoAs', 'Gulo', 'nosA', 'rosA', 'Aves', 'EChO', 'beet', 'mino', 'IpsA', 'AphA', 'BIAs', 'nasa', 'DURA', 'katA', 'taro', 'iDAs', 'SutA', 'aria', 'Ulex', 'ZUMA', 'Rapa', 'Pusa', 'rohu', 'orca', 'metA', 'COPA', 'puma', 'aphA', 'DoGs', 'MalO', 'malO', 'vela', 'pusa', 'kale', 'cura', 'isiA', 'mago', 'pilA', 'GOYA', 'SULA', 'INCA', 'AKis', 'SINA', 'MINO', 'RapA', 'rapA', 'ColO', 'MedA', 'LumA', 'Unio', 'apha', 'MOMA', 'MoPs', 'sagA', 'miCe', 'MUSA', 'Deer', 'mink', 'MICa', 'agnA', 'AgnA', 'Zebu', 'zebu', 'Dama', 'ucla', 'NaSA', 'ColA', 'CoLa', 'medA', 'GLIA', 'ASGV', 'PSEN', 'Kava', 'peas', 'ipsa']\n"
     ]
    }
   ],
   "source": [
    "print(four_letter_pass['Source Term'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6724db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "ExT2T_stat['number of 5 letter terms extracted'] = len(five_letter_matches['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['5 letter term list'] = five_letter_matches['original mapped term'].unique().tolist()\n",
    "print(len(five_letter_matches['original mapped term'].unique().tolist()))\n",
    "five_letter_pass = five_letter_matches[five_letter_matches['Original Mapping Score']>0.95]\n",
    "ExT2T_stat['5 letter terms with >0.95 match via T2T'] = five_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 5 letter terms after T2T filter'] = len(five_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(five_letter_pass['Source Term'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "46d77ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'mouse', 'swine', 'flies', 'birds', 'nanos', 'wheat', 'maize', 'venus', 'goats', 'lamia', 'lambs', 'sheep', 'fungi', 'apple', 'valsa', 'helix', 'frogs', 'smaug', 'codon', 'cocoa', 'galea', 'wasps', 'turbo', 'alexa', 'aroma', 'horse', 'hydra', 'ticks', 'areca', 'cacao', 'lemon', 'danio', 'mucor', 'bream', 'hippa', 'phoma', 'panda', 'ramie', 'ciona', 'paris', 'radix', 'boars', 'goose', 'geese', 'acari', 'malus', 'peach', 'thada', 'triso', 'gemma', 'acris', 'moths', 'janus', 'aegis', 'onion', 'equus', 'krill', 'aedes', 'culex', 'disco', 'geron', 'nihon', 'pinna', 'arida', 'tiger', 'agria', 'psara', 'anise', 'thyme', 'dingo', 'aotus', 'gadus', 'dorea', 'tampa', 'apela', 'eland', 'ducks', 'bears', 'vitis', 'tulsa', 'monza', 'bursa', 'oryza', 'taxus', 'morel', 'mixta', 'babax', 'pears', 'seals', 'fleas', 'arima', 'camel', 'theba', 'fagus', 'vinca', 'clove', 'lotus', 'ixora', 'midge', 'gesta', 'seila', 'palea', 'peria', 'delta', 'xenia', 'llama', 'lethe', 'pecan', 'perro', 'cornu', 'conta', 'rumex', 'mucoa', 'soter', 'takin', 'phago', 'samba', 'larra', 'hevea', 'axion', 'polia', 'sars2', 'anura', 'newts', 'pisum', 'picea']\n"
     ]
    }
   ],
   "source": [
    "five_pass = five_letter_pass['Source Term'].unique().tolist()\n",
    "print(five_pass)\n",
    "with open(os.path.join(output_path,'five_letter_pass.tsv'),'w') as outlist:\n",
    "    for eachword in five_pass:\n",
    "        outlist.write(eachword+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16e334ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExT2T_stat['3 letter false positives after filter'] = 48-12\n",
    "ExT2T_stat['4 letter false positives after filter'] = 181-42\n",
    "ExT2T_stat['5 letter false positives after filter'] = 36\n",
    "ExT2T_stat['5 letter true positives that are standalone terms'] = 47\n",
    "ExT2T_stat['5 letter true positives that are NOT standalone terms (part of another term)'] = 41\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_filter_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(ExT2T_stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7c610",
   "metadata": {},
   "source": [
    "# Heuristics for improving accuracy based on the results of the EXTRACT text mapping\n",
    "\n",
    "1. Text2Term is weaker at mapping terms with fewer letters so a score cut off should be applied for terms with 3 letters, 4 letters, and 5 letters\n",
    "    * For 3 letter terms\n",
    "      * The capitalization will not affect the match terms\n",
    "      * A match score of >0.95 is likely to be correct\n",
    "      * However, the number of extracted terms that are likely to be true terms vs false positives is only 12/48 even after meeting this threshhold. For this reason, we should exclude terms of 3 or less characters as the percentage EXTRACTed correctly is pretty low\n",
    "    * For 4 letter terms\n",
    "      * At a score of >0.95, cats, rats and bats will be false negatives (matched well, but dropped), while duck (matched a little too specifically to domestic duck) will pass\n",
    "      * At a score of >0.91, cats, rats, and bats will pass, duck will still be an issue\n",
    "      * However, the number of extracted terms that are likely to be true terms is only ~42/181. For this reason, we should also exclude terms of 4 or less characters\n",
    "    * For 5 letter terms\n",
    "      * At a score of >0.95, the number of true positives is 91/127\n",
    "      * Of those 91 true positive terms, about 41 are terms which are part of a taxonomic phrase (i.e. - either only the genus part of a taxonomy or a species part\n",
    "      * This means that only about 41 of these terms would be missed with the number of letters threshhold for EXTRACT were >5, the remaining true positives would likely be captured with the whole term\n",
    "2. Text2Term is also weaker at mapping terms formatted as g. species, as it can score higher mapping to the correct species term but incorrect genus. To address this:\n",
    "    * Identify such terms using regex (r\"\\b[A-Z]\\.\\s[^\\s]+\\b\")\n",
    "    * For these terms, only take the result if the genus letter matches the first letter of the mapped result (i.e. split on '.', take first)\n",
    "    * There will be plenty of exceptions, but this should address the majority of problematic matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d0ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
