{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a03664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsueng\\Anaconda3\\envs\\nde\\lib\\site-packages\\pydantic\\_internal\\_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'underscore_attrs_are_private' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import text2term\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b495586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-01 12:43:04 INFO [text2term.term_collector]: Loading ontology https://purl.obolibrary.org/obo/ncbitaxon.owl...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtext2term\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://purl.obolibrary.org/obo/ncbitaxon.owl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mncbitaxon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\text2term\\t2t.py:118\u001b[0m, in \u001b[0;36mcache_ontology\u001b[1;34m(ontology_url, ontology_acronym, base_iris)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ontology_acronym \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    117\u001b[0m     ontology_acronym \u001b[38;5;241m=\u001b[39m ontology_url\n\u001b[1;32m--> 118\u001b[0m ontology_terms \u001b[38;5;241m=\u001b[39m \u001b[43m_load_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_iris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_deprecated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOntologyTermType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mANY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m, ontology_acronym)\n\u001b[0;32m    120\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching ontology \u001b[39m\u001b[38;5;132;01m{\u001b[39;00montology_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\text2term\\t2t.py:186\u001b[0m, in \u001b[0;36m_load_ontology\u001b[1;34m(ontology, iris, exclude_deprecated, use_cache, term_type)\u001b[0m\n\u001b[0;32m    184\u001b[0m     onto_terms \u001b[38;5;241m=\u001b[39m filter_terms(onto_terms_unfiltered, iris, exclude_deprecated, term_type)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     term_collector \u001b[38;5;241m=\u001b[39m \u001b[43mOntologyTermCollector\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_iri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montology\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     onto_terms \u001b[38;5;241m=\u001b[39m term_collector\u001b[38;5;241m.\u001b[39mget_ontology_terms(base_iris\u001b[38;5;241m=\u001b[39miris, exclude_deprecated\u001b[38;5;241m=\u001b[39mexclude_deprecated,\n\u001b[0;32m    188\u001b[0m                                                    term_type\u001b[38;5;241m=\u001b[39mterm_type)\n\u001b[0;32m    189\u001b[0m     term_collector\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\text2term\\term_collector.py:19\u001b[0m, in \u001b[0;36mOntologyTermCollector.__init__\u001b[1;34m(self, ontology_iri, use_reasoning, log_level)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03mConstruct an ontology term collector for the ontology at the given IRI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m:param ontology_iri: IRI of the ontology (e.g., path of ontology document in the local file system, URL)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m:param use_reasoning: Use a reasoner to compute inferred class hierarchy\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m onto_utils\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m, level\u001b[38;5;241m=\u001b[39mlog_level)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39montology \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_iri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reasoning:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classify_ontology(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39montology)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\text2term\\term_collector.py:346\u001b[0m, in \u001b[0;36mOntologyTermCollector._load_ontology\u001b[1;34m(self, ontology_iri)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m owl_link \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     ontology_iri \u001b[38;5;241m=\u001b[39m owl_link\n\u001b[1;32m--> 346\u001b[0m ontology \u001b[38;5;241m=\u001b[39m \u001b[43mget_ontology\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_iri\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_ontology_metrics(ontology)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\namespace.py:966\u001b[0m, in \u001b[0;36mOntology.load\u001b[1;34m(self, only_local, fileobj, reload, reload_if_newer, url, **args)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OwlReadyOntologyParsingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot download \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (url \u001b[38;5;129;01mor\u001b[39;00m f))\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m   new_base_iri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mparse(fileobj, default_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_base_iri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OwlReadyOntologyParsingError:\n\u001b[0;32m    968\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.owl\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.rdf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m url: \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\driver.py:264\u001b[0m, in \u001b[0;36mBaseSubGraph.parse\u001b[1;34m(self, f, format, delete_existing_triples, default_base)\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     queue \u001b[38;5;241m=\u001b[39m _FakeQueue(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimport_triples_from_queue(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), delete_existing_triples))\n\u001b[1;32m--> 264\u001b[0m     onto_base_iri \u001b[38;5;241m=\u001b[39m \u001b[43mdo_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m800000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OwlReadyOntologyParsingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    267\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_obj_triple_raw_spo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monto\u001b[38;5;241m.\u001b[39mstorid, rdf_type, owl_ontology)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\driver.py:233\u001b[0m, in \u001b[0;36mBaseSubGraph.parse.<locals>.do_parse\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrdfxml\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    232\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mowlready2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdfxml_2_ntriples\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m   \u001b[43mowlready2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdfxml_2_ntriples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_prepare_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_prepare_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mowlready2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mowlxml_2_ntriples\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\rdfxml_2_ntriples.py:313\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(f, on_prepare_obj, on_prepare_data, new_blank, default_base)\u001b[0m\n\u001b[0;32m    311\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    312\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    315\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m OwlReadyOntologyParsingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDF/XML parsing error in file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, line \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, column \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m???\u001b[39m\u001b[38;5;124m\"\u001b[39m)), parser\u001b[38;5;241m.\u001b[39mCurrentLineNumber, parser\u001b[38;5;241m.\u001b[39mCurrentColumnNumber)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\b\\abs_5csrwf7hpg\\croot\\python-split_1679423826728\\work\\modules\\pyexpat.c:470\u001b[0m, in \u001b[0;36mEndElement\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\rdfxml_2_ntriples.py:262\u001b[0m, in \u001b[0;36mparse.<locals>.endElement\u001b[1;34m(tag)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m   parse_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    261\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fake_bn(iri):\n\u001b[1;32m--> 262\u001b[0m     \u001b[43mon_prepare_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43miri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_bn(iri):\n\u001b[0;32m    265\u001b[0m     add_to_bn(iri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREL\u001b[39m\u001b[38;5;124m\"\u001b[39m, tag, value)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\driver.py:226\u001b[0m, in \u001b[0;36mBaseSubGraph.parse.<locals>.do_parse.<locals>.on_prepare_obj\u001b[1;34m(*triple)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m objs\n\u001b[0;32m    225\u001b[0m objs\u001b[38;5;241m.\u001b[39mappend(triple)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m30000\u001b[39m: \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobjs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m;  objs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\driver.py:51\u001b[0m, in \u001b[0;36m_FakeQueue.put\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m     50\u001b[0m   command, triples \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m   command \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m:   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m command \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatas\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_datas(triples)\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01melif\u001b[39;00m command \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\triplelite.py:910\u001b[0m, in \u001b[0;36mSubGraph.import_triples_from_queue.<locals>.insert_objs\u001b[1;34m(triples)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_objs\u001b[39m(triples):\n\u001b[1;32m--> 910\u001b[0m   objs \u001b[38;5;241m=\u001b[39m [(_abbreviate(s), _abbreviate(p), _abbreviate(o)) \u001b[38;5;28;01mfor\u001b[39;00m s, p, o \u001b[38;5;129;01min\u001b[39;00m triples]\n\u001b[0;32m    911\u001b[0m   cur\u001b[38;5;241m.\u001b[39mexecutemany(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT OR IGNORE INTO objs VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,?,?,?)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, objs)\n\u001b[0;32m    912\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m new_abbrevs:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\triplelite.py:910\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_objs\u001b[39m(triples):\n\u001b[1;32m--> 910\u001b[0m   objs \u001b[38;5;241m=\u001b[39m [(\u001b[43m_abbreviate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m, _abbreviate(p), _abbreviate(o)) \u001b[38;5;28;01mfor\u001b[39;00m s, p, o \u001b[38;5;129;01min\u001b[39;00m triples]\n\u001b[0;32m    911\u001b[0m   cur\u001b[38;5;241m.\u001b[39mexecutemany(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT OR IGNORE INTO objs VALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,?,?,?)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc, objs)\n\u001b[0;32m    912\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m new_abbrevs:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\triplelite.py:895\u001b[0m, in \u001b[0;36mSubGraph.import_triples_from_queue.<locals>._abbreviate\u001b[1;34m(iri)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m iri\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# A blank node\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     storid \u001b[38;5;241m=\u001b[39m abbrevs[iri] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_blank_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m     r \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT storid FROM resources WHERE iri=? LIMIT 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, (iri,))\u001b[38;5;241m.\u001b[39mfetchone()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\site-packages\\owlready2\\triplelite.py:424\u001b[0m, in \u001b[0;36mGraph.new_blank_node\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_blank_node\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 424\u001b[0m   blank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT current_blank+1 FROM store\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchone()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    425\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPDATE store SET current_blank=?\u001b[39m\u001b[38;5;124m\"\u001b[39m, (blank,))\n\u001b[0;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mblank\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text2term.cache_ontology(\"https://purl.obolibrary.org/obo/ncbitaxon.owl\", \"ncbitaxon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ed2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                                species  \n",
      "0  [{'name': 'Caenorhabditis elegans'}]  \n",
      "1  [{'name': 'Caenorhabditis elegans'}]  \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data','raw_species_results.pickle'),'rb') as infile:\n",
    "    raw_species = pickle.load(infile)\n",
    "\n",
    "print(raw_species.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26afc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200837\n",
      "202257\n",
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                              species  \n",
      "0  {'name': 'Caenorhabditis elegans'}  \n",
      "1  {'name': 'Caenorhabditis elegans'}  \n"
     ]
    }
   ],
   "source": [
    "print(len(raw_species))\n",
    "\n",
    "raw_boom = raw_species.explode('species')\n",
    "print(len(raw_boom))\n",
    "print(raw_boom.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3154fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id       _ignored  _score  \\\n",
      "0  GEO_GSE110840  [all.keyword]     1.0   \n",
      "1  GEO_GSE110842  [all.keyword]     1.0   \n",
      "\n",
      "                                                name  \\\n",
      "0  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "                              species            species_name  flag_raised  \n",
      "0  {'name': 'Caenorhabditis elegans'}  Caenorhabditis elegans        False  \n",
      "1  {'name': 'Caenorhabditis elegans'}  Caenorhabditis elegans        False  \n"
     ]
    }
   ],
   "source": [
    "def pop_species(species_dict):\n",
    "    if isinstance(species_dict,dict):\n",
    "        x = species_dict['name']\n",
    "    elif isinstance(species_dict,str):\n",
    "        x = species_dict\n",
    "    return x\n",
    "\n",
    "## Raise a flag if the species was not formatted as a dictionary\n",
    "def flag_species(species_dict):\n",
    "    if isinstance(species_dict,dict):\n",
    "        x = False\n",
    "    elif isinstance(species_dict,str):\n",
    "        x = True\n",
    "    return x    \n",
    "\n",
    "raw_boom['species_name'] = raw_boom.apply(lambda row: pop_species(row['species']),axis=1)\n",
    "raw_boom['flag_raised'] = raw_boom.apply(lambda row: flag_species(row['species']),axis=1)\n",
    "print(raw_boom.head(n=2))\n",
    "clean_dict = raw_boom[['_id','name','species_name','flag_raised']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97115ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             _id                                               name  \\\n",
      "0  GEO_GSE110840  Sequencing of Caenorhabditis elegans deletion ...   \n",
      "1  GEO_GSE110842  Sequencing of Caenorhabditis elegans overexpre...   \n",
      "\n",
      "             species_name  flag_raised  \n",
      "0  Caenorhabditis elegans        False  \n",
      "1  Caenorhabditis elegans        False  \n"
     ]
    }
   ],
   "source": [
    "print(clean_dict.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae479db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2565\n"
     ]
    }
   ],
   "source": [
    "no_dups = clean_dict['species_name'].unique().tolist()\n",
    "print(len(no_dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b41a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 10:38:58 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-13 10:41:30 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-13 10:41:30 INFO [text2term.t2t]: Mapping 2565 source terms to ncbitaxon\n",
      "2023-11-13 10:46:52 INFO [text2term.t2t]: ...done (mapping time: 321.58s seconds)\n",
      "CPU times: total: 7min 57s\n",
      "Wall time: 8min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t2t_result = text2term.map_terms(no_dups, \"ncbitaxon\", use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d7b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Source Term ID    Source Term  \\\n",
      "3180  http://ccb.hms.harvard.edu/t2t/R3Go4L6RwRz  Abies pinsapo   \n",
      "3181  http://ccb.hms.harvard.edu/t2t/R3Go4L6RwRz  Abies pinsapo   \n",
      "\n",
      "               Mapped Term Label Mapped Term CURIE  \\\n",
      "3180               Abies pinsapo   NCBITAXON:56046   \n",
      "3181  Abies pinsapo var. pinsapo  NCBITAXON:928732   \n",
      "\n",
      "                                      Mapped Term IRI  Mapping Score  Tags  \n",
      "3180   http://purl.obolibrary.org/obo/NCBITaxon_56046          0.997  None  \n",
      "3181  http://purl.obolibrary.org/obo/NCBITaxon_928732          0.923  None  \n"
     ]
    }
   ],
   "source": [
    "t2t_result.sort_values(['Source Term','Mapping Score'], ascending = [True,False], inplace=True)\n",
    "print(t2t_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f529b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2564\n"
     ]
    }
   ],
   "source": [
    "top_score = t2t_result.drop_duplicates('Source Term',keep='first')\n",
    "print(len(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a08e5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_dictdf = top_score[['Source Term','Mapped Term Label','Mapped Term CURIE', 'Mapping Score']].copy()\n",
    "t2t_dictdf.rename(columns={'Source Term':'species_name', 'Mapped Term CURIE':'CURIE','Mapped Term Label':'Label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b25ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202256\n",
      "202182\n"
     ]
    }
   ],
   "source": [
    "mapped_dict = clean_dict.merge(t2t_dictdf, on='species_name', how='inner')\n",
    "print(len(mapped_dict))\n",
    "mapped_dict.drop_duplicates(keep='first', inplace=True)\n",
    "print(len(mapped_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "610b5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197509\n"
     ]
    }
   ],
   "source": [
    "no_flags = mapped_dict.loc[mapped_dict['flag_raised']==False]\n",
    "flagged = mapped_dict.loc[mapped_dict['flag_raised']==True]\n",
    "print(len(no_flags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e173a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_flags.to_csv(os.path.join('data','t2t_mapped_no_flags.tsv'), sep='\\t', header=True)\n",
    "flagged.to_csv(os.path.join('data','t2t_mapped_flagged.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100bfd7",
   "metadata": {},
   "source": [
    "## Process the data from the EXTRACT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996687e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set filepaths\n",
    "script_path = os.getcwd()\n",
    "parent_path = os.path.dirname(script_path)\n",
    "input_path = os.path.join(parent_path,'EXTRACT_check','data')\n",
    "input_file = os.path.join(input_path,'test_100000.tsv')\n",
    "output_path = os.path.join(script_path,'data')\n",
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd92c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210242\n",
      "         _id extracted_term  extracted_type  taxid           CURIE\n",
      "0  GSE238109         bovine              -2   9913  NCBITAXON:9913\n",
      "1  GSE187829          human              -2   9606  NCBITAXON:9606\n"
     ]
    }
   ],
   "source": [
    "extract_results = pd.read_csv(input_file,delimiter='\\t',header=0,index_col=0)\n",
    "print(len(extract_results))\n",
    "print(extract_results.head(n=2))\n",
    "stats['number of results from EXTRACT']=len(extract_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4c768fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(extract_results.loc[(extract_results['extracted_type']!=-2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e99139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8851\n",
      "['bovine', 'human', 'mouse', 'mice', 'murine', 'Human', 'P. gingivalis', 'Porphyromonas gingivalis', 'Mouse', 'MCMV']\n"
     ]
    }
   ],
   "source": [
    "unique_terms = extract_results['extracted_term'].unique().tolist()\n",
    "print(len(unique_terms))\n",
    "print(unique_terms[0:10])\n",
    "stats['number of unique extracted terms']=len(unique_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb1068",
   "metadata": {},
   "source": [
    "### Check if casing matters in processing of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be48692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7727\n",
      "['artemisia afra', 'p. yoelii nigeriensis', 'pyrus pyrifolia', 'lined ground squirrels', 'anabaena pcc7120', 'potato psyllid', 'caenorhabditis elegans', 'anopheles moucheti', 'b. germanica', 'b. bacteriovorus']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type datetime is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent change if casing adjusted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(unique_terms)\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(lower_unique)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_terms))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2t_EXTRACT_test.json\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outwrite:\n\u001b[1;32m---> 12\u001b[0m     outwrite\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\nde\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type datetime is not JSON serializable"
     ]
    }
   ],
   "source": [
    "## Note capitalization introduces duplication; however, proper casing can be useful in species determination\n",
    "## We'll also test whether or not capitalization affects Text2Term mappings\n",
    "lower_unique = list(set([x.lower() for x in unique_terms]))\n",
    "print(len(lower_unique))\n",
    "print(lower_unique[0:10])\n",
    "\n",
    "## The lower casing and de-duplication removed only about 12% of the terms\n",
    "stats['number of unique terms if casing adjusted (to lower case)']= len(lower_unique)\n",
    "stats['percent change if casing adjusted'] = (len(unique_terms)-len(lower_unique)/len(unique_terms))\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d158e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.016336\n",
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418}\n"
     ]
    }
   ],
   "source": [
    "uniquestarttime = datetime.now()\n",
    "time.sleep(1)\n",
    "uniqueendtime = datetime.now()\n",
    "duration = uniqueendtime-uniquestarttime\n",
    "print(duration)\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1476ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418}\n",
      "2023-11-18 08:49:02 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 08:49:44 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 08:49:44 INFO [text2term.t2t]: Mapping 8851 source terms to ncbitaxon\n",
      "2023-11-18 08:54:48 INFO [text2term.t2t]: ...done (mapping time: 303.78s seconds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:16\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_type' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)\n",
    "uniquestarttime = datetime.now()\n",
    "stats['Time start for T2T test on unique terms (casing not adjusted)'] = datetime.strftime(uniquestarttime,'%m/%d/%Y, %H:%M:%S')\n",
    "\n",
    "unique_t2t_result = text2term.map_terms(unique_terms, \"ncbitaxon\", use_cache=True)\n",
    "\n",
    "uniqueendtime = datetime.now()\n",
    "stats['Time end for T2T test on unique terms (casing not adjusted)'] = datetime.strftime(uniqueendtime,'%m/%d/%Y, %H:%M:%S')\n",
    "stats['Duration of T2T test on unique terms (casing not adjusted)'] = str(uniqueendtime - uniquestarttime)\n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_unique_t2t_test.pickle'),'wb') as outfile:\n",
    "    pickle.dump(unique_t2t_result,outfile)\n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a577509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number of results from EXTRACT': 210242, 'number of unique extracted terms': 8851, 'number of unique terms if casing adjusted (to lower case)': 7727, 'percent change if casing adjusted': 8850.126991300418, 'Time start for T2T test on unique terms (casing not adjusted)': '11/18/2023, 08:49:02', 'Time end for T2T test on unique terms (casing not adjusted)': '11/18/2023, 08:57:08', 'Duration of T2T test on unique terms (casing not adjusted)': '0:08:06.170061'}\n",
      "2023-11-18 10:16:59 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 10:17:54 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 10:17:54 INFO [text2term.t2t]: Mapping 7727 source terms to ncbitaxon\n",
      "2023-11-18 10:23:27 INFO [text2term.t2t]: ...done (mapping time: 332.62s seconds)\n",
      "CPU times: total: 8min 34s\n",
      "Wall time: 9min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'r') as statfile:\n",
    "    stats = json.loads(statfile.read())\n",
    "    print(stats)\n",
    "\n",
    "lowerstarttime = datetime.now()\n",
    "stats['Time start for T2T test on unique terms (casing adjusted)'] = datetime.strftime(lowerstarttime,'%m/%d/%Y, %H:%M:%S')\n",
    "\n",
    "lower_t2t_result = text2term.map_terms(lower_unique, \"ncbitaxon\", use_cache=True)\n",
    "\n",
    "lowerendtime = datetime.now()\n",
    "stats['Time end for T2T test on unique terms (casing adjusted)'] = datetime.strftime(lowerendtime,'%m/%d/%Y, %H:%M:%S')\n",
    "stats['Duration of T2T test on unique terms (casing adjusted)'] = str(lowerendtime - lowerstarttime)\n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_lower_t2t_test.pickle'),'wb') as loweroutfile:\n",
    "    pickle.dump(lower_t2t_result,loweroutfile)  \n",
    "\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45c30b",
   "metadata": {},
   "source": [
    "#### Actually run the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0961e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Compare processing of case-adjusted vs original case mappings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutput_path\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXTRACT_lower_t2t_test.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m loweroutfile:\n\u001b[0;32m      3\u001b[0m     lower_t2t_result \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(loweroutfile)  \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXTRACT_unique_t2t_test.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "## Compare processing of case-adjusted vs original case mappings\n",
    "with open(os.path.join(output_path,'EXTRACT_lower_t2t_test.pickle'),'rb') as loweroutfile:\n",
    "    lower_t2t_result = pickle.load(loweroutfile)  \n",
    "\n",
    "with open(os.path.join(output_path,'EXTRACT_unique_t2t_test.pickle'),'rb') as outfile:\n",
    "    unique_t2t_result = pickle.load(outfile) \n",
    "    \n",
    "## rename columns and add lower case column in preparation for a merge\n",
    "unique_t2t_result.rename(columns={'Source Term':'original mapped term','Mapping Score':'Original Mapping Score'}, inplace=True)\n",
    "unique_t2t_result['Source Term'] = [x.lower() for x in unique_t2t_result['original mapped term']]\n",
    "\n",
    "lower_t2t_result.rename(columns={'Mapping Score':'Lower Case Map Score'},inplace = True)\n",
    "\n",
    "print(unique_t2t_result.head(n=2))\n",
    "print(lower_t2t_result.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "553b19e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8844\n",
      "7720\n",
      "8678\n",
      "    original mapped term   Source Term   Mapped Term Label  Mapped Term CURIE  \\\n",
      "71            A. linaria    a. linaria     Linaria <birds>  NCBITAXON:1930290   \n",
      "108         A. stephensi  a. stephensi  Stephensia <moths>  NCBITAXON:1073692   \n",
      "134                 abaM          abam       Musa textilis   NCBITAXON:228183   \n",
      "246                Aedes         aedes    Aedes <subgenus>   NCBITAXON:149531   \n",
      "\n",
      "     Original Mapping Score  Lower Case Map Score  \n",
      "71                    0.938                   NaN  \n",
      "108                   0.823                   NaN  \n",
      "134                   0.495                   NaN  \n",
      "246                   0.999                   NaN  \n"
     ]
    }
   ],
   "source": [
    "unique2merge = unique_t2t_result[['original mapped term','Source Term','Mapped Term Label','Mapped Term CURIE','Original Mapping Score']].copy()\n",
    "unique2merge.sort_values(['Source Term','Original Mapping Score'], ascending=[True,False],inplace=True)\n",
    "unique2merge.drop_duplicates(subset=['original mapped term','Source Term'],keep='first',inplace=True)\n",
    "\n",
    "lower2merge = lower_t2t_result[['Source Term','Mapped Term Label','Mapped Term CURIE','Lower Case Map Score']].copy()\n",
    "lower2merge.sort_values(['Source Term','Lower Case Map Score'], ascending=[True,False],inplace=True)\n",
    "lower2merge.drop_duplicates(subset=['Source Term'],keep='first',inplace=True)\n",
    "print(len(unique2merge))\n",
    "print(len(lower2merge))\n",
    "\n",
    "mergeddf = unique2merge.merge(lower2merge,on=['Source Term','Mapped Term Label','Mapped Term CURIE'],how='left')\n",
    "matchedmergeddf = unique2merge.merge(lower2merge,on=['Source Term','Mapped Term Label','Mapped Term CURIE'],how='inner')\n",
    "print(len(matchedmergeddf))\n",
    "\n",
    "lower_map_fail = mergeddf.loc[mergeddf['Lower Case Map Score'].isna()]\n",
    "print(lower_map_fail.head(n=4))\n",
    "\n",
    "## inspect some of the failures to see if any improvements can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d93d8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227\n",
      "2877\n",
      "4574\n",
      "              original mapped term                      Source Term  \\\n",
      "2                            5z489                            5z489   \n",
      "3                           A- RNA                           a- rna   \n",
      "4                            A-RNA                            a-rna   \n",
      "9  A. actinomycetemcomitans HK1651  a. actinomycetemcomitans hk1651   \n",
      "\n",
      "                              Mapped Term Label  Mapped Term CURIE  \\\n",
      "2                              Enytus sp. Oz489  NCBITAXON:2071065   \n",
      "3                                 RNA virus sp.  NCBITAXON:2217650   \n",
      "4                                 RNA virus sp.  NCBITAXON:2217650   \n",
      "9  Aggregatibacter actinomycetemcomitans HK1651   NCBITAXON:272556   \n",
      "\n",
      "   Original Mapping Score  Lower Case Map Score  \n",
      "2                   0.448                 0.447  \n",
      "3                   0.756                 0.755  \n",
      "4                   0.756                 0.755  \n",
      "9                   0.816                 0.815  \n",
      "=================\n",
      "  original mapped term    Source Term  \\\n",
      "0            2019-nCoV      2019-ncov   \n",
      "1            2019_nCoV      2019_ncov   \n",
      "6        A.  fumigatus  a.  fumigatus   \n",
      "7             A. aceti       a. aceti   \n",
      "\n",
      "                                 Mapped Term Label  Mapped Term CURIE  \\\n",
      "0  Severe acute respiratory syndrome coronavirus 2  NCBITAXON:2697049   \n",
      "1  Severe acute respiratory syndrome coronavirus 2  NCBITAXON:2697049   \n",
      "6             Aspergillus fumigatus var. fumigatus    NCBITAXON:41122   \n",
      "7                                Acetobacter aceti      NCBITAXON:435   \n",
      "\n",
      "   Original Mapping Score  Lower Case Map Score  \n",
      "0                   0.598                 0.601  \n",
      "1                   0.598                 0.601  \n",
      "6                   0.844                 0.845  \n",
      "7                   0.783                 0.784  \n",
      "=================\n",
      "        original mapped term               Source Term  \\\n",
      "5                      a-TEA                     a-tea   \n",
      "8   A. actinomycetemcomitans  a. actinomycetemcomitans   \n",
      "10              A. aculeatus              a. aculeatus   \n",
      "12                A. aegypti                a. aegypti   \n",
      "\n",
      "                        Mapped Term Label  Mapped Term CURIE  \\\n",
      "5                             Tea A virus  NCBITAXON:2941489   \n",
      "8   Aggregatibacter actinomycetemcomitans      NCBITAXON:714   \n",
      "10       Gasterosteus aculeatus aculeatus   NCBITAXON:481459   \n",
      "12                  Aedes aegypti aegypti  NCBITAXON:1424507   \n",
      "\n",
      "    Original Mapping Score  Lower Case Map Score  \n",
      "5                    0.832                 0.832  \n",
      "8                    0.748                 0.748  \n",
      "10                   0.838                 0.838  \n",
      "12                   0.915                 0.915  \n"
     ]
    }
   ],
   "source": [
    "## Check if score for terms which were mapped twice are higher or lower based on casing\n",
    "original_higher = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']>matchedmergeddf['Lower Case Map Score']]\n",
    "lower_case_higher = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']<matchedmergeddf['Lower Case Map Score']]\n",
    "scores_equal = matchedmergeddf.loc[matchedmergeddf['Original Mapping Score']==matchedmergeddf['Lower Case Map Score']]\n",
    "print(len(original_higher))\n",
    "print(len(lower_case_higher))\n",
    "print(len(scores_equal))\n",
    "\n",
    "print(original_higher.head(n=4))\n",
    "print('=================')\n",
    "print(lower_case_higher.head(n=4))\n",
    "print('=================')\n",
    "print(scores_equal.head(n=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474259b",
   "metadata": {},
   "source": [
    "lower casing does not save too much time, so preserve it in order to ensure good capture of species in the [Letter. word] format\n",
    "\n",
    "In some cases, T2T will give a match to the species term a higher score even though it's in the wrong genus. To select the right match, it may be necessary to pull these cases out by regex and select the match that includes the correct genus abbreviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60964a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A.  fumigatus', 'A. aceti', 'A. actinomycetemcomitans', 'A. actinomycetemcomitans HK1651', 'A. aculeatus', 'A. adenophora', 'A. aegypti', 'A. afra', 'A. afraspera', 'A. alternata']\n",
      "1781\n"
     ]
    }
   ],
   "source": [
    "term_w_periods = [x for x in unique2merge['original mapped term'] if '.' in x]\n",
    "print(term_w_periods[0:10])\n",
    "print(len(term_w_periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ff92890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 11:02:14 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 11:03:02 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 11:03:02 INFO [text2term.t2t]: Mapping 1781 source terms to ncbitaxon\n",
      "2023-11-18 11:06:43 INFO [text2term.t2t]: ...done (mapping time: 220.19s seconds)\n",
      "5343\n"
     ]
    }
   ],
   "source": [
    "t2t_period = text2term.map_terms(term_w_periods, \"ncbitaxon\", use_cache=True)\n",
    "print(len(t2t_period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1cae215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Source Term ID             Source Term  \\\n",
      "5333  http://ccb.hms.harvard.edu/t2t/RGgMK2pniYm              Z. mobilis   \n",
      "5334  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5335  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5336  http://ccb.hms.harvard.edu/t2t/RAGfzuYznxQ              Z. tritici   \n",
      "5337  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5338  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5339  http://ccb.hms.harvard.edu/t2t/R7b9FtTLSvs      Zea mays ssp. mays   \n",
      "5340  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "5341  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "5342  http://ccb.hms.harvard.edu/t2t/R3kh9Pt9xxm  Zea mays ssp. mexicana   \n",
      "\n",
      "                                    Mapped Term Label  Mapped Term CURIE  \\\n",
      "5333                                 Bacillus mobilis  NCBITAXON:2026190   \n",
      "5334                                       Triticinae  NCBITAXON:1648030   \n",
      "5335                              Pseudomonas tritici  NCBITAXON:2745518   \n",
      "5336                                  Anguina tritici   NCBITAXON:166006   \n",
      "5337                                         Zea mays     NCBITAXON:4577   \n",
      "5338                             Zea mays subsp. mays   NCBITAXON:381124   \n",
      "5339  Zea mays subsp. mays x Zea mays subsp. mexicana  NCBITAXON:3053866   \n",
      "5340                         Zea mays subsp. mexicana     NCBITAXON:4579   \n",
      "5341  Zea mays subsp. mays x Zea mays subsp. mexicana  NCBITAXON:3053866   \n",
      "5342                                         Zea mays     NCBITAXON:4577   \n",
      "\n",
      "                                       Mapped Term IRI  Mapping Score  Tags  \n",
      "5333  http://purl.obolibrary.org/obo/NCBITaxon_2026190          0.710  None  \n",
      "5334  http://purl.obolibrary.org/obo/NCBITaxon_1648030          0.674  None  \n",
      "5335  http://purl.obolibrary.org/obo/NCBITaxon_2745518          0.645  None  \n",
      "5336   http://purl.obolibrary.org/obo/NCBITaxon_166006          0.636  None  \n",
      "5337     http://purl.obolibrary.org/obo/NCBITaxon_4577          0.897  None  \n",
      "5338   http://purl.obolibrary.org/obo/NCBITaxon_381124          0.886  None  \n",
      "5339  http://purl.obolibrary.org/obo/NCBITaxon_3053866          0.807  None  \n",
      "5340     http://purl.obolibrary.org/obo/NCBITaxon_4579          0.845  None  \n",
      "5341  http://purl.obolibrary.org/obo/NCBITaxon_3053866          0.751  None  \n",
      "5342     http://purl.obolibrary.org/obo/NCBITaxon_4577          0.666  None  \n"
     ]
    }
   ],
   "source": [
    "t2t_period\n",
    "print(t2t_period.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0a569",
   "metadata": {},
   "source": [
    "Otherwise, Text2Term performs quite well and the source of error is usually EXTRACT\n",
    "\n",
    "Many NER tools fail for shorter length terms simply due to increased ambiguation. For EXTRACT, determine if a threshhold number of characters should be used to limit the number of false positives introduced by EXTRACT\n",
    "\n",
    "Inspect the results of EXTRACT at three and four characters to see if a filter on the Text2Term match score can help limit the false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c03877d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 12:03:46 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-18 12:04:36 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-18 12:04:36 INFO [text2term.t2t]: Mapping 41 source terms to ncbitaxon\n",
      "2023-11-18 12:07:06 INFO [text2term.t2t]: ...done (mapping time: 149.81s seconds)\n",
      "41 123\n",
      "CPU times: total: 3min 18s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "three_letter_list = ['cat','dog','owl','fox','elk','cow','pig','hen','ant','rat','eel','bat','ewe','emu','yak','sow',\n",
    "                     'CAT','Dog','OWL','Fox','Elk','Cow','Pig','Hen','Ant','Rat','Eel','Bat','Ewe','Emu','Yak','Sow',\n",
    "                     'eye','hit','man','boy','car','ass','but','why','Moo']\n",
    "three_letter_match = text2term.map_terms(three_letter_list, \"ncbitaxon\", use_cache=True)\n",
    "print(len(three_letter_list),len(three_letter_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85e21847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Source Term ID Source Term Mapped Term Label  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/R4M5P5PZMSo         cat       Felis catus   \n",
      "1  http://ccb.hms.harvard.edu/t2t/R4M5P5PZMSo         cat    Catopuma badia   \n",
      "\n",
      "  Mapped Term CURIE                                 Mapped Term IRI  \\\n",
      "0    NCBITAXON:9685   http://purl.obolibrary.org/obo/NCBITaxon_9685   \n",
      "1   NCBITAXON:61454  http://purl.obolibrary.org/obo/NCBITaxon_61454   \n",
      "\n",
      "   Mapping Score  Tags  \n",
      "0          0.960  None  \n",
      "1          0.632  None  \n"
     ]
    }
   ],
   "source": [
    "print(three_letter_match.head(n=2))\n",
    "three_letter_match.to_csv(os.path.join(output_path,'three_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "479c0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-20 07:00:12 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 07:00:52 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 07:00:52 INFO [text2term.t2t]: Mapping 20 source terms to ncbitaxon\n",
      "2023-11-20 07:02:49 INFO [text2term.t2t]: ...done (mapping time: 116.83s seconds)\n",
      "20 60\n",
      "CPU times: total: 2min 37s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "four_letter_list = ['lion','goat','deer','bear','seal','cats','dogs','bats','Rats','Wolf','duck',\n",
    "                    'Hare','toad','Vole','Crab','clam','kids','lynx','LCMV','MRSA']\n",
    "\n",
    "four_letter_match = text2term.map_terms(four_letter_list, \"ncbitaxon\", use_cache=True)\n",
    "print(len(four_letter_list),len(four_letter_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff86858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_letter_match.to_csv(os.path.join(output_path,'four_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab062122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "                                 Source Term ID original mapped term  \\\n",
      "6    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "7    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "8    http://ccb.hms.harvard.edu/t2t/R4TETiNZMQ7                mouse   \n",
      "108  http://ccb.hms.harvard.edu/t2t/R7Eh7VS57VF                Yeast   \n",
      "109  http://ccb.hms.harvard.edu/t2t/R7Eh7VS57VF                Yeast   \n",
      "\n",
      "             Mapped Term Label Mapped Term CURIE  \\\n",
      "6                 Mus musculus   NCBITAXON:10090   \n",
      "7                  Mus <genus>   NCBITAXON:10088   \n",
      "8             Polyplax serrata  NCBITAXON:468196   \n",
      "108  Saccharomyces pastorianus   NCBITAXON:27292   \n",
      "109         Pichia sp. Yeast 2  NCBITAXON:553839   \n",
      "\n",
      "                                     Mapped Term IRI  Original Mapping Score  \\\n",
      "6     http://purl.obolibrary.org/obo/NCBITaxon_10090                   0.999   \n",
      "7     http://purl.obolibrary.org/obo/NCBITaxon_10088                   0.999   \n",
      "8    http://purl.obolibrary.org/obo/NCBITaxon_468196                   0.852   \n",
      "108   http://purl.obolibrary.org/obo/NCBITaxon_27292                   0.793   \n",
      "109  http://purl.obolibrary.org/obo/NCBITaxon_553839                   0.758   \n",
      "\n",
      "     Tags Source Term  \n",
      "6    None       mouse  \n",
      "7    None       mouse  \n",
      "8    None       mouse  \n",
      "108  None       yeast  \n",
      "109  None       yeast  \n"
     ]
    }
   ],
   "source": [
    "five_letter_matches = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==5]\n",
    "## There are ~847 rows to review for five letter matches, reduce the number for evaluation\n",
    "five_letter_sample = random.sample(five_letter_matches['original mapped term'].unique().tolist(),33)\n",
    "five_letter_subset = five_letter_matches.loc[five_letter_matches['original mapped term'].isin(five_letter_sample)]\n",
    "\n",
    "print(len(five_letter_subset))\n",
    "print(five_letter_subset.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce6a3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_letter_subset.to_csv(os.path.join(output_path,'five_letter_thresh.tsv'), sep='\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ef608",
   "metadata": {},
   "source": [
    "## EXTRACT character limit test\n",
    "Looking at the sampled data for five letters, it looks like Text2Term is doing a good job mapping, but the false positive rate from EXTRACT is affecting the results. Check to see the likelihood of false positives getting dropped AFTER a text2term matching due to low score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8ddd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "['rat', 'HIV', 'pig', 'p22', 'TIV', 'Rat', 'HCV', 'ape', 'dog', 'TCV', 'HeV', 'MLV', 'IAV', 'MYA', 'UCA', 'HBV', 'HPV', 'cow', 'TEV', 'IPs', 'GH1', 'e16', 'RSV', 'osa', 'mcf', 'EBV', 'CMV', 'yak', 'Bcg', 'MAV', 'P22', 'mCG', 'SSP', 'NNV', 'RRV', 'NT1', 'TBV', 'mac', 'EOS', 'rye', 'iDA', 'sIA', 'MTb', 'HEV', 'SAG', 'AKA', 'GAS', 'HOA', 'vir', 'WMV', 'chp', 'JEV', 'Eos', 'NDV', 'ULA', 'e11', 'SFs', 'MCG', 'TSV', 'Doa', 'ATV', 'DWV', 'ALV', 'bee', 'HH1', 'eos', 'AON', 'AMV', 'BLV', 'VZV', 'pax', 'YFV', 'Aon', 'VIR', 'CB1', 'SUS', 'elk', 'Bpv', 'ChP', 'DCV', 'SP5', 'RKN', 'SOa', 'RDV', 'kob', 'Cow', 'SMV', 'e17', 'SV5', 'Sp4', 'Ips', 'Oar', 'MVV', 'ami', 'e19', 'PVY', 'ula', 'PMV', 'DMV', 'pMV', 'Sus', 'AS5', 'IBV', 'SCV', 'ECC', 'ITi', 'UCa', 'DOA', 'AOA', 'cEV', 'Vir', 'HDV', 'mAC', 'mev', 'LMV', 'BDV', 'isa', 'ena', 'SpV', 'TOs', 'Hpb', 'Sag', 'DRV', 'YMW', 'HEA', 'MEV', 'MPV', 'BEV', 'NMV', 'mCF', 'IpA', 'PrV', 'PVX', 'TMV', 'sav', 'FPV', 'IVE', 'FV3', 'BOA', 'QIA', 'hH1', 'UFO', 'MvA', 'ips', 'Gh1', 'TRV', 'mTB', 'oar', 'BKV', 'UTA', 'scV', 'PoA', 'tos', 'STV', 'Yak', 'ufo', 'TAV', 'pTV', 'IFV']\n",
      "2023-11-20 08:06:26 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 08:07:06 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 08:07:06 INFO [text2term.t2t]: Mapping 159 source terms to ncbitaxon\n",
      "2023-11-20 08:08:58 INFO [text2term.t2t]: ...done (mapping time: 111.75s seconds)\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ExT2T_stat = {}\n",
    "three_letter_EXTRACT = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==3]\n",
    "ExT2T_stat['number of 3 letter terms extracted'] = len(three_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['3 letter term list'] = three_letter_EXTRACT['original mapped term'].unique().tolist()\n",
    "print(len(three_letter_EXTRACT['original mapped term'].unique().tolist()))\n",
    "print(three_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "three_letter_check = text2term.map_terms(three_letter_EXTRACT['original mapped term'].unique().tolist(), \"ncbitaxon\", use_cache=True)\n",
    "three_letter_pass = three_letter_check.loc[three_letter_check['Mapping Score']>0.95]\n",
    "ExT2T_stat['3 letter terms with >0.95 match via T2T'] = three_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 3 letter terms after T2T filter'] = len(three_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(three_letter_pass['Source Term'].unique().tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca613370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rat', 'pig', 'Rat', 'ape', 'dog', 'MYA', 'UCA', 'cow', 'IPs', 'osa', 'yak', 'EOS', 'rye', 'sIA', 'AKA', 'HOA', 'Eos', 'ULA', 'Doa', 'bee', 'eos', 'AON', 'pax', 'Aon', 'elk', 'SOa', 'kob', 'Cow', 'Ips', 'Oar', 'ami', 'ula', 'ITi', 'UCa', 'DOA', 'AOA', 'isa', 'ena', 'HEA', 'IpA', 'IVE', 'UFO', 'ips', 'oar', 'UTA', 'PoA', 'Yak', 'ufo']\n"
     ]
    }
   ],
   "source": [
    "print(three_letter_pass['Source Term'].unique().tolist())\n",
    "## At a match score of > 0.95, the percentage of EXTRACT terms that appear to be true terms is \n",
    "## 12 / 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97ec23f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "2023-11-20 08:14:23 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-11-20 08:14:54 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-11-20 08:14:54 INFO [text2term.t2t]: Mapping 370 source terms to ncbitaxon\n",
      "2023-11-20 08:16:41 INFO [text2term.t2t]: ...done (mapping time: 107.13s seconds)\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "four_letter_EXTRACT = unique_t2t_result.loc[unique_t2t_result['original mapped term'].astype(str).str.len()==4]\n",
    "ExT2T_stat['number of 4 letter terms extracted'] = len(four_letter_EXTRACT['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['4 letter term list'] = four_letter_EXTRACT['original mapped term'].unique().tolist()\n",
    "print(len(four_letter_EXTRACT['original mapped term'].unique().tolist()))\n",
    "four_letter_check = text2term.map_terms(four_letter_EXTRACT['original mapped term'].unique().tolist(), \"ncbitaxon\", use_cache=True)\n",
    "four_letter_pass = four_letter_check.loc[four_letter_check['Mapping Score']>0.95]\n",
    "ExT2T_stat['4 letter terms with >0.95 match via T2T'] = four_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 4 letter terms after T2T filter'] = len(four_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(four_letter_pass['Source Term'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19010d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mice', 'Mice', 'H1N1', 'rice', 'flax', 'apes', 'rats', 'dogs', 'inti', 'corn', 'sigA', 'GAGA', 'SAGA', 'goat', 'H3N2', 'Babr', 'aRNA', 'H5N1', 'pupa', 'LIMe', 'betA', 'atlA', 'PUMA', 'MOPS', 'Glia', 'MICA', 'Rats', 'pear', 'bees', 'ARIA', 'IOBA', 'Pima', 'tasA', 'TasA', 'coma', 'aloe', 'topi', 'PilA', 'lice', 'Loma', 'H7N9', 'carp', 'oats', 'Ugni', 'moPs', 'MyLa', 'MACE', 'CreX', 'MOAs', 'Homo', 'SaGA', 'eNOS', 'duck', 'LamA', 'mASO', 'NatA', 'Clea', 'Apis', 'AtlA', 'SigA', 'RANA', 'lion', 'nusA', 'H9N2', 'Bees', 'PISA', 'naso', 'CLEA', 'Goat', 'SIMO', 'ArcA', 'ENOS', 'SigE', 'Lice', 'TAMU', 'THOR', 'MetA', 'PATU', 'RITA', 'netA', 'NetA', 'plum', 'alcA', 'Pupa', 'aroA', 'AroA', 'LAMA', 'MicA', 'nepA', 'disA', 'neem', 'rana', 'MeTA', 'sigE', 'Ucla', 'aliA', 'afer', 'rapa', 'pacu', 'hemp', 'arcA', 'pLUM', 'ANIA', 'aniA', 'deer', 'thor', 'MoAs', 'Gulo', 'nosA', 'rosA', 'Aves', 'EChO', 'beet', 'mino', 'IpsA', 'AphA', 'BIAs', 'nasa', 'DURA', 'katA', 'taro', 'iDAs', 'SutA', 'aria', 'Ulex', 'ZUMA', 'Rapa', 'Pusa', 'rohu', 'orca', 'metA', 'COPA', 'puma', 'aphA', 'DoGs', 'MalO', 'malO', 'vela', 'pusa', 'kale', 'cura', 'isiA', 'mago', 'pilA', 'GOYA', 'SULA', 'INCA', 'AKis', 'SINA', 'MINO', 'RapA', 'rapA', 'ColO', 'MedA', 'LumA', 'Unio', 'apha', 'MOMA', 'MoPs', 'sagA', 'miCe', 'MUSA', 'Deer', 'mink', 'MICa', 'agnA', 'AgnA', 'Zebu', 'zebu', 'Dama', 'ucla', 'NaSA', 'ColA', 'CoLa', 'medA', 'GLIA', 'ASGV', 'PSEN', 'Kava', 'peas', 'ipsa']\n"
     ]
    }
   ],
   "source": [
    "print(four_letter_pass['Source Term'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fb801598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "ExT2T_stat['number of 5 letter terms extracted'] = len(five_letter_matches['original mapped term'].unique().tolist())\n",
    "ExT2T_stat['5 letter term list'] = five_letter_matches['original mapped term'].unique().tolist()\n",
    "print(len(five_letter_matches['original mapped term'].unique().tolist()))\n",
    "five_letter_pass = five_letter_matches[five_letter_matches['Original Mapping Score']>0.95]\n",
    "ExT2T_stat['5 letter terms with >0.95 match via T2T'] = five_letter_pass['Source Term'].unique().tolist()\n",
    "ExT2T_stat['number of 5 letter terms after T2T filter'] = len(five_letter_pass['Source Term'].unique().tolist())\n",
    "print(len(five_letter_pass['Source Term'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6e1ac78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'mouse', 'swine', 'flies', 'birds', 'nanos', 'wheat', 'maize', 'venus', 'goats', 'lamia', 'lambs', 'sheep', 'fungi', 'apple', 'valsa', 'helix', 'frogs', 'smaug', 'codon', 'cocoa', 'galea', 'wasps', 'turbo', 'alexa', 'aroma', 'horse', 'hydra', 'ticks', 'areca', 'cacao', 'lemon', 'danio', 'mucor', 'bream', 'hippa', 'phoma', 'panda', 'ramie', 'ciona', 'paris', 'radix', 'boars', 'goose', 'geese', 'acari', 'malus', 'peach', 'thada', 'triso', 'gemma', 'acris', 'moths', 'janus', 'aegis', 'onion', 'equus', 'krill', 'aedes', 'culex', 'disco', 'geron', 'nihon', 'pinna', 'arida', 'tiger', 'agria', 'psara', 'anise', 'thyme', 'dingo', 'aotus', 'gadus', 'dorea', 'tampa', 'apela', 'eland', 'ducks', 'bears', 'vitis', 'tulsa', 'monza', 'bursa', 'oryza', 'taxus', 'morel', 'mixta', 'babax', 'pears', 'seals', 'fleas', 'arima', 'camel', 'theba', 'fagus', 'vinca', 'clove', 'lotus', 'ixora', 'midge', 'gesta', 'seila', 'palea', 'peria', 'delta', 'xenia', 'llama', 'lethe', 'pecan', 'perro', 'cornu', 'conta', 'rumex', 'mucoa', 'soter', 'takin', 'phago', 'samba', 'larra', 'hevea', 'axion', 'polia', 'sars2', 'anura', 'newts', 'pisum', 'picea']\n"
     ]
    }
   ],
   "source": [
    "five_pass = five_letter_pass['Source Term'].unique().tolist()\n",
    "print(five_pass)\n",
    "with open(os.path.join(output_path,'five_letter_pass.tsv'),'w') as outlist:\n",
    "    for eachword in five_pass:\n",
    "        outlist.write(eachword+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9524d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _id extracted_term  extracted_type  taxid           CURIE\n",
      "0  GSE238109         bovine              -2   9913  NCBITAXON:9913\n",
      "1  GSE187829          human              -2   9606  NCBITAXON:9606\n",
      "16260\n",
      "326\n",
      "['bovine', 'murine', 'Murine', 'rodent', 'medaka', 'Mo-MLV', 'peanut', 'plants', 'humans', 'monkey', 'E.coli', 'Tomato', 'cattle', 'Bovine', 'pineal', 'poplar', 'Cattle', 'Humans', 'chicks', 'E coli', 'binary', 'Aglaia', 'citrus', 'Potato', 'potato', 'IPO323', 'Plants', 'A- RNA', 'rabbit', 'fishes', 'Causal', 'tomato', 'bursas', 'TH9197', 'HTLV-1', 'baboon', 'cotton', 'cardia', 'aphids', 'Rodent', 'Vibrio', 'Fishes', 'barley', 'cereal', 'Macaca', 'Blasti', 'Xenops', 'aromas', 'puncta', 'Monkey', 'Tuxedo', 'quinoa', 'tunica', 'almond', 'name A', 'DISEAE', 'amoeba', 'Binary', 'Allium', 'garlic', 'MN_ASO', 'horses', 'equine', 'Rabbit', 'Peanut', 'salmon', 'pinnal', 'pinnae', 'EV-A71', 'turkey', 'Aerial', 'Pichia', 'HPV-16', 'HTLV-I', 'singal', 'CHIRON', 'Merial', 'Ixodes', 'chilli', 'canola', 'TOMATO', 'AURORA', 'Pineal', 'Citrus', 'lagena', 'CNEP-A', 'vervet', 'BOVINE', 'Agouti', 'septal', 'baliga', 'Livial', 'dicots', 'Late-S', 'Equine', 'GaHV-1', 'rattus', 'Banana', 'banana', 'Septal', 'C Beta', 'turbot', 'Spombe', 'two LF', 'erecta', 'CYRANO', 'STELLA', 'Striga', 'Anolis', 'Medaka', 'yarrow', 'OvHV-2', 'AlHV-1', 'DHAV-1', 'ferret', 'Ferret', 'turnip', 'Varroa', 'longan', 'Nostoc', 'Daucus', 'anneal', 'Bombyx', 'HPV 16', 'meadia', 'Anoxia', 'mastic', 'COSMOS', 'lychee', 'litchi', 'Lychee', 'HERV-K', 'snakes', 'Aphids', 'Cyrano', 'Turbot', 'crassa', 'Garlic', 'CA USA', 'Fmouse', 'agouti', 'scopal', 'Prunus', 'Bamboo', 'radula', 'chiton', 'Costal', 'costal', 'PE RNA', 'HPV 11', 'labium', 'oryzae', 'Afipia', 'Latina', 'd Apis', 'rachis', 'avenae', 'cowpea', 'Cowpea', 'Quinoa', 'hu-man', 'AZA RA', 'A. cod', 'ARA VA', 'Jun co', 'Poplar', 'G. max', 'celery', 'noduli', 'MES-SA', 'adonis', 'Sonora', 'castor', 'bamboo', 'iguana', 'pandas', 'mi  ce', 'Horses', 'MP_0_3', 'Gemmae', 'skates', 'parvum', 'enemas', 'HYbrid', 'PRRSV1', 'Albugo', 'CAMERA', 'ev-d68', 'EV-D68', 'Danaus', 'Amoeba', 'Leuzea', 'papaya', 'medfly', 'D Beta', 'emeral', 'tuxedo', 'MHV-68', 'gollum', 'Gollum', 'Volvox', 'PanaMa', 'N-Exos', 'gibbon', 'scolex', 'metria', 'ginkgo', 'Cellia', 'b-wave', 'deltas', 'Osiris', 'vibrio', 'hg   2', 'telata', 'Ocimum', 'Mvenus', 'bursal', 'DELTAs', 'carrot', 'koalas', 'CV-A10', 'Delima', 'taenea', 'diseae', 'fennel', 'R etli', 'C or A', 'ginger', 'B upon', 'Lilium', 'Lmo co', 'Obelus', 'SIVagm', 'MIDEAS', 'paRNAs', 'sharks', 'l HEMA', 'CV-A16', 'radish', 'MuHV-4', 'OsHV-1', 'discus', 'CBS138', 'REMORA', 'acidon', 'Thisbe', 'croton', 'Nevada', 'olives', 'Pandas', 'J. Fox', 'Bursal', 'sterna', 'HIV- 1', 'STINGa', 'myotis', 'Labial', 'AcMNPV', 'catlas', 'Late S', 'H. zea', 'C afer', 'Apidae', 'cCol I', 'Nano-S', 'Cereal', 'EQUINE', 'Rothia', 'acacia', 'varroa', 'Brugia', 'bowfin', 'geckos', 'ALLIUM', 'pitaya', 'MARINa', 'BARLEY', 'PVYNTN', 'thrips', 'PS  vs', 'Oryzae', 'S.suis', 'Rattus', 'one GA', 'SAFE R', 'aronia', 'SD-ura', 'PHASIS', 'Persea', 'A. koa', 'CBS767', 'Line I', 'Clytia', 'Indigo', 'Bremia', 'pomelo', 'Pomelo', 'Arnica', 'host A', 'JUN co', 'Litchi', 'SA via', 'durian', 'how EA', 'phleum', 'donkey', 'HERV-W', 'coccal', 'rPolII', 'B.rapa', 'semial', 'MdSGHV', 'HUMANS', 'Melb-a', 'canole', 'Pistia', 'pistia', 'sole N', 'shrews', 'gne WT', 'Gene A', 'C-beta', 'ALL_IA', 'Kminus', 'cornus', 'WT   2']\n",
      "2023-12-01 12:52:27 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-01 12:53:05 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-01 12:53:05 INFO [text2term.t2t]: Mapping 326 source terms to ncbitaxon\n",
      "2023-12-01 12:55:18 INFO [text2term.t2t]: ...done (mapping time: 133.35s seconds)\n"
     ]
    }
   ],
   "source": [
    "print(extract_results.head(n=2))\n",
    "six_char = extract_results.loc[extract_results['extracted_term'].astype(str).str.len()==6]\n",
    "print(len(six_char))\n",
    "unique_six = six_char['extracted_term'].unique().tolist()\n",
    "print(len(unique_six))\n",
    "print(unique_six)\n",
    "t2t_six = text2term.map_terms(unique_six, \"ncbitaxon\", use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b97177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Source Term ID Source Term Mapped Term Label  \\\n",
      "24  http://ccb.hms.harvard.edu/t2t/REhoGvq7MQz      humans              Homo   \n",
      "25  http://ccb.hms.harvard.edu/t2t/REhoGvq7MQz      humans      Homo sapiens   \n",
      "26  http://ccb.hms.harvard.edu/t2t/REhoGvq7MQz      humans           Humaria   \n",
      "\n",
      "   Mapped Term CURIE                                  Mapped Term IRI  \\\n",
      "24    NCBITAXON:9605    http://purl.obolibrary.org/obo/NCBITaxon_9605   \n",
      "25    NCBITAXON:9606    http://purl.obolibrary.org/obo/NCBITaxon_9606   \n",
      "26  NCBITAXON:137251  http://purl.obolibrary.org/obo/NCBITaxon_137251   \n",
      "\n",
      "    Mapping Score  Tags  \n",
      "24          0.999  None  \n",
      "25          0.721  None  \n",
      "26          0.546  None  \n",
      "2023-12-01 13:06:11 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-01 13:06:46 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-01 13:06:46 INFO [text2term.t2t]: Mapping 2 source terms to ncbitaxon\n",
      "2023-12-01 13:08:53 INFO [text2term.t2t]: ...done (mapping time: 126.59s seconds)\n",
      "                               Source Term ID Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/R5KJvWfkCsY    COVID-19   \n",
      "1  http://ccb.hms.harvard.edu/t2t/R5KJvWfkCsY    COVID-19   \n",
      "2  http://ccb.hms.harvard.edu/t2t/R5KJvWfkCsY    COVID-19   \n",
      "3  http://ccb.hms.harvard.edu/t2t/R6exDmkeHUo  SARS-CoV-2   \n",
      "4  http://ccb.hms.harvard.edu/t2t/R6exDmkeHUo  SARS-CoV-2   \n",
      "5  http://ccb.hms.harvard.edu/t2t/R6exDmkeHUo  SARS-CoV-2   \n",
      "\n",
      "                                 Mapped Term Label  Mapped Term CURIE  \\\n",
      "0  Severe acute respiratory syndrome coronavirus 2  NCBITAXON:2697049   \n",
      "1                                   Porina covidii  NCBITAXON:2874915   \n",
      "2                                    Echovirus E19    NCBITAXON:47507   \n",
      "3    Severe acute respiratory syndrome coronavirus  NCBITAXON:2901879   \n",
      "4                            Bat SARS CoV Rf1/2004   NCBITAXON:347537   \n",
      "5                            Bat SARS CoV Rm1/2004   NCBITAXON:347536   \n",
      "\n",
      "                                    Mapped Term IRI  Mapping Score  Tags  \n",
      "0  http://purl.obolibrary.org/obo/NCBITaxon_2697049          0.499  None  \n",
      "1  http://purl.obolibrary.org/obo/NCBITaxon_2874915          0.437  None  \n",
      "2    http://purl.obolibrary.org/obo/NCBITaxon_47507          0.416  None  \n",
      "3  http://purl.obolibrary.org/obo/NCBITaxon_2901879          0.650  None  \n",
      "4   http://purl.obolibrary.org/obo/NCBITaxon_347537          0.566  None  \n",
      "5   http://purl.obolibrary.org/obo/NCBITaxon_347536          0.563  None  \n"
     ]
    }
   ],
   "source": [
    "#print(t2t_six.head(n=2))\n",
    "print(t2t_six.loc[t2t_six['Source Term']=='humans'])\n",
    "print(text2term.map_terms(['COVID-19', 'SARS-CoV-2'], \"ncbitaxon\", use_cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944f4f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 12:16:41 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-04 12:17:16 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-04 12:17:16 INFO [text2term.t2t]: Mapping 1 source terms to ncbitaxon\n",
      "2023-12-04 12:19:11 INFO [text2term.t2t]: ...done (mapping time: 115.15s seconds)\n",
      "                               Source Term ID        Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/RDuiVprasKp  Anopheles gambiae   \n",
      "1  http://ccb.hms.harvard.edu/t2t/RDuiVprasKp  Anopheles gambiae   \n",
      "2  http://ccb.hms.harvard.edu/t2t/RDuiVprasKp  Anopheles gambiae   \n",
      "\n",
      "                          Mapped Term Label  Mapped Term CURIE  \\\n",
      "0                         Anopheles gambiae     NCBITAXON:7165   \n",
      "1       Anopheles gambiae x Anopheles merus  NCBITAXON:1547548   \n",
      "2  Anopheles gambiae x Anopheles arabiensis  NCBITAXON:1975581   \n",
      "\n",
      "                                    Mapped Term IRI  Mapping Score  Tags  \n",
      "0     http://purl.obolibrary.org/obo/NCBITaxon_7165          0.984  None  \n",
      "1  http://purl.obolibrary.org/obo/NCBITaxon_1547548          0.899  None  \n",
      "2  http://purl.obolibrary.org/obo/NCBITaxon_1975581          0.833  None  \n"
     ]
    }
   ],
   "source": [
    "print(text2term.map_terms(['Anopheles gambiae'], \"ncbitaxon\", use_cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "be46b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExT2T_stat['3 letter false positives after filter'] = 48-12\n",
    "ExT2T_stat['4 letter false positives after filter'] = 181-42\n",
    "ExT2T_stat['5 letter false positives after filter'] = 36\n",
    "ExT2T_stat['5 letter true positives that are standalone terms'] = 47\n",
    "ExT2T_stat['5 letter true positives that are NOT standalone terms (part of another term)'] = 41\n",
    "with open(os.path.join(output_path,'t2t_EXTRACT_filter_test.json'),'w') as outwrite:\n",
    "    outwrite.write(json.dumps(ExT2T_stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652eac6",
   "metadata": {},
   "source": [
    "### Investigate locations and other potential common sources of error getting mapped by T2T\n",
    "\n",
    "Certain locations exist as species names in the NCBI Taxonomy. Text2Term will happily map anything that is pulled out by EXTRACT, and is not able to readily distinguish if a term mentioned was actually a species or if it was isolated from a specific location. Given the likelihood of mentions for locations to be much higher than the mentions of very specific species with the same name as those locations, we can improve the accuracy of the results by simply dropping all extracted and mapped location names.\n",
    "\n",
    "Additionally, there may be other terms that frequently appear that are likely to represent something other than a species mention. Those terms which will consistently be picked up by EXTRACT and successfully mapped by T2T should also be dropped if the likelihood of them being irrelevant is very high relative to the likelihood of them being correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbab248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " China\n",
      "                                  item   itemLabel\n",
      "0   http://www.wikidata.org/entity/Q99  California\n",
      "1  http://www.wikidata.org/entity/Q173     Alabama\n",
      "                                 item itemLabel\n",
      "0  http://www.wikidata.org/entity/Q16    Canada\n",
      "1  http://www.wikidata.org/entity/Q17     Japan\n"
     ]
    }
   ],
   "source": [
    "## checking location mappings\n",
    "## EXTRACT successfully pulled out the following locations from a list as a species term: Tonga, Nevada, Montana\n",
    "## EXTRACT did not pull China from this list, but it is possible it may do it \n",
    "## This test will look for exact matches for location names in NCBI Taxonomy using Text2Term\n",
    "def clean_stopwords(termtext):\n",
    "    stopwordlist = [\"People's Republic of\",\"State of\",\"Kingdom of\", \"Republic of the\", \"Republic of\",\"Republic\",\"Democratic\"]\n",
    "    for eachword in stopwordlist:\n",
    "        termtext = termtext.replace(eachword,\"\")\n",
    "    return termtext\n",
    "\n",
    "print(clean_stopwords(\"People's Republic of China\"))\n",
    "\n",
    "states = pd.read_csv(os.path.join(input_path,'state_query_results.tsv'), delimiter='\\t',header=0)\n",
    "print(states.head(n=2))\n",
    "countries = pd.read_csv(os.path.join(input_path,'country_query_results.tsv'), delimiter='\\t',header=0)\n",
    "print(countries.head(n=2))\n",
    "\n",
    "countrylist = countries['itemLabel'].unique().tolist()\n",
    "raw_text = \"\"\n",
    "cleancountry = [clean_stopwords(eachcountry) for eachcountry in countrylist]\n",
    "statelist = states['itemLabel'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c67595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11 11:13:48 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-11 11:14:26 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-11 11:14:27 INFO [text2term.t2t]: Mapping 201 source terms to ncbitaxon\n",
      "2023-12-11 11:17:08 INFO [text2term.t2t]: ...done (mapping time: 161.51s seconds)\n",
      "2023-12-11 11:17:14 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-11 11:18:01 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-11 11:18:01 INFO [text2term.t2t]: Mapping 50 source terms to ncbitaxon\n",
      "2023-12-11 11:20:08 INFO [text2term.t2t]: ...done (mapping time: 127.07s seconds)\n",
      "                               Source Term ID Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/RBXLrKyzzZ8  California   \n",
      "1  http://ccb.hms.harvard.edu/t2t/RBXLrKyzzZ8  California   \n",
      "\n",
      "               Mapped Term Label Mapped Term CURIE  \\\n",
      "0                     California  NCBITAXON:337343   \n",
      "1  California encephalitis virus   NCBITAXON:35305   \n",
      "\n",
      "                                   Mapped Term IRI  Mapping Score  Tags  \n",
      "0  http://purl.obolibrary.org/obo/NCBITaxon_337343          0.994  None  \n",
      "1   http://purl.obolibrary.org/obo/NCBITaxon_35305          0.915  None  \n",
      "CPU times: total: 6min 11s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t2t_country = text2term.map_terms(cleancountry, \"ncbitaxon\", use_cache=True)\n",
    "t2t_state = text2term.map_terms(statelist, \"ncbitaxon\", use_cache=True)\n",
    "print(t2t_state.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ed46ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tonga', 'Alabama', 'Nevada', 'Argentina', 'Namibia', 'Panama', 'Virginia', 'Bulgaria', 'Togo', ' China', 'Serendip', 'Arizona', 'California']\n"
     ]
    }
   ],
   "source": [
    "t2t_state.sort_values(by=['Source Term','Mapping Score'],ascending=False,inplace=True)\n",
    "state_top = t2t_state.drop_duplicates(subset='Source Term', keep='first')\n",
    "state_matches = state_top.loc[state_top['Source Term'].str.strip()==state_top['Mapped Term Label'].str.strip()]\n",
    "#print(state_matches)\n",
    "t2t_state.sort_values(by=['Source Term','Mapping Score'],ascending=False,inplace=True)\n",
    "country_top = t2t_country.drop_duplicates(subset='Source Term', keep='first')\n",
    "country_matches = country_top.loc[country_top['Source Term'].str.strip()==country_top['Mapped Term Label'].str.strip()]\n",
    "#print(country_matches)\n",
    "\n",
    "location_drop_list = list(set(state_matches['Source Term'].tolist()).union(set(country_matches['Source Term'].to_list())))\n",
    "print(location_drop_list)\n",
    "drop_dict = {'locations':location_drop_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32f9d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11 13:23:49 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-11 13:24:27 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-11 13:24:27 INFO [text2term.t2t]: Mapping 3 source terms to ncbitaxon\n",
      "2023-12-11 13:26:13 INFO [text2term.t2t]: ...done (mapping time: 106.71s seconds)\n",
      "                               Source Term ID  Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/RFPvG6yBMh6    SARSCoV-2   \n",
      "1  http://ccb.hms.harvard.edu/t2t/RFPvG6yBMh6    SARSCoV-2   \n",
      "2  http://ccb.hms.harvard.edu/t2t/RFPvG6yBMh6    SARSCoV-2   \n",
      "3  http://ccb.hms.harvard.edu/t2t/R5JKbHc4mFJ  coronavirus   \n",
      "4  http://ccb.hms.harvard.edu/t2t/R5JKbHc4mFJ  coronavirus   \n",
      "5  http://ccb.hms.harvard.edu/t2t/R5JKbHc4mFJ  coronavirus   \n",
      "6  http://ccb.hms.harvard.edu/t2t/RC4yTrAwAyZ   SARS-CoV-2   \n",
      "7  http://ccb.hms.harvard.edu/t2t/RC4yTrAwAyZ   SARS-CoV-2   \n",
      "8  http://ccb.hms.harvard.edu/t2t/RC4yTrAwAyZ   SARS-CoV-2   \n",
      "\n",
      "                               Mapped Term Label  Mapped Term CURIE  \\\n",
      "0                    Expression vector SARSCoV1S  NCBITAXON:2819196   \n",
      "1                    Expression vector SARSCoV2S  NCBITAXON:2819197   \n",
      "2                                         Sarsia     NCBITAXON:6078   \n",
      "3                                Bat coronavirus  NCBITAXON:1508220   \n",
      "4                                Rat coronavirus    NCBITAXON:31632   \n",
      "5                              Meles coronavirus  NCBITAXON:2971612   \n",
      "6  Severe acute respiratory syndrome coronavirus  NCBITAXON:2901879   \n",
      "7                          Bat SARS CoV Rf1/2004   NCBITAXON:347537   \n",
      "8                          Bat SARS CoV Rm1/2004   NCBITAXON:347536   \n",
      "\n",
      "                                    Mapped Term IRI  Mapping Score  Tags  \n",
      "0  http://purl.obolibrary.org/obo/NCBITaxon_2819196          0.421  None  \n",
      "1  http://purl.obolibrary.org/obo/NCBITaxon_2819197          0.421  None  \n",
      "2     http://purl.obolibrary.org/obo/NCBITaxon_6078          0.410  None  \n",
      "3  http://purl.obolibrary.org/obo/NCBITaxon_1508220          0.797  None  \n",
      "4    http://purl.obolibrary.org/obo/NCBITaxon_31632          0.783  None  \n",
      "5  http://purl.obolibrary.org/obo/NCBITaxon_2971612          0.772  None  \n",
      "6  http://purl.obolibrary.org/obo/NCBITaxon_2901879          0.643  None  \n",
      "7   http://purl.obolibrary.org/obo/NCBITaxon_347537          0.578  None  \n",
      "8   http://purl.obolibrary.org/obo/NCBITaxon_347536          0.575  None  \n"
     ]
    }
   ],
   "source": [
    "print(text2term.map_terms(['SARSCoV-2','coronavirus','SARS-CoV-2'], \"ncbitaxon\", use_cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac48830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11 13:13:49 INFO [text2term.t2t]: Loading cached ontology from: cache\\ncbitaxon\\ncbitaxon-term-details.pickle\n",
      "2023-12-11 13:14:27 INFO [text2term.t2t]: Filtered ontology terms to those of type: class\n",
      "2023-12-11 13:14:27 INFO [text2term.t2t]: Mapping 3 source terms to ncbitaxon\n",
      "2023-12-11 13:17:09 INFO [text2term.t2t]: ...done (mapping time: 162.31s seconds)\n",
      "                               Source Term ID     Source Term  \\\n",
      "0  http://ccb.hms.harvard.edu/t2t/R8Zhgf4WWeL  gut microbiome   \n",
      "1  http://ccb.hms.harvard.edu/t2t/R8Zhgf4WWeL  gut microbiome   \n",
      "2  http://ccb.hms.harvard.edu/t2t/R8Zhgf4WWeL  gut microbiome   \n",
      "3  http://ccb.hms.harvard.edu/t2t/RAN69Vc7KHh          chicks   \n",
      "4  http://ccb.hms.harvard.edu/t2t/RAN69Vc7KHh          chicks   \n",
      "5  http://ccb.hms.harvard.edu/t2t/RAN69Vc7KHh          chicks   \n",
      "6  http://ccb.hms.harvard.edu/t2t/RFdm4qD75Q9  gut+microbiome   \n",
      "7  http://ccb.hms.harvard.edu/t2t/RFdm4qD75Q9  gut+microbiome   \n",
      "8  http://ccb.hms.harvard.edu/t2t/RFdm4qD75Q9  gut+microbiome   \n",
      "\n",
      "      Mapped Term Label  Mapped Term CURIE  \\\n",
      "0            Microbiota    NCBITAXON:13613   \n",
      "1             Microtome  NCBITAXON:1766024   \n",
      "2  mouse gut metagenome   NCBITAXON:410661   \n",
      "3               Ixodida     NCBITAXON:6935   \n",
      "4         Gallus gallus     NCBITAXON:9031   \n",
      "5           Chichicaste   NCBITAXON:256719   \n",
      "6            Microbiota    NCBITAXON:13613   \n",
      "7             Microtome  NCBITAXON:1766024   \n",
      "8  mouse gut metagenome   NCBITAXON:410661   \n",
      "\n",
      "                                    Mapped Term IRI  Mapping Score  Tags  \n",
      "0    http://purl.obolibrary.org/obo/NCBITaxon_13613          0.568  None  \n",
      "1  http://purl.obolibrary.org/obo/NCBITaxon_1766024          0.557  None  \n",
      "2   http://purl.obolibrary.org/obo/NCBITaxon_410661          0.501  None  \n",
      "3     http://purl.obolibrary.org/obo/NCBITaxon_6935          0.599  None  \n",
      "4     http://purl.obolibrary.org/obo/NCBITaxon_9031          0.556  None  \n",
      "5   http://purl.obolibrary.org/obo/NCBITaxon_256719          0.504  None  \n",
      "6    http://purl.obolibrary.org/obo/NCBITaxon_13613          0.568  None  \n",
      "7  http://purl.obolibrary.org/obo/NCBITaxon_1766024          0.557  None  \n",
      "8   http://purl.obolibrary.org/obo/NCBITaxon_410661          0.501  None  \n"
     ]
    }
   ],
   "source": [
    "print(text2term.map_terms(['gut microbiome','chicks','gut+microbiome'], \"ncbitaxon\", use_cache=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ae7fa",
   "metadata": {},
   "source": [
    "# Heuristics for improving accuracy based on the results of the EXTRACT text mapping\n",
    "\n",
    "1. Text2Term is weaker at mapping terms with fewer letters so a score cut off should be applied for terms with 3 letters, 4 letters, and 5 letters\n",
    "    * For 3 letter terms\n",
    "      * The capitalization will not affect the match terms\n",
    "      * A match score of >0.95 is likely to be correct\n",
    "      * However, the number of extracted terms that are likely to be true terms vs false positives is only 12/48 even after meeting this threshhold. For this reason, we should exclude terms of 3 or less characters as the percentage EXTRACTed correctly is pretty low\n",
    "    * For 4 letter terms\n",
    "      * At a score of >0.95, cats, rats and bats will be false negatives (matched well, but dropped), while duck (matched a little too specifically to domestic duck) will pass\n",
    "      * At a score of >0.91, cats, rats, and bats will pass, duck will still be an issue\n",
    "      * However, the number of extracted terms that are likely to be true terms is only ~42/181. For this reason, we should also exclude terms of 4 or less characters\n",
    "    * For 5 letter terms\n",
    "      * At a score of >0.95, the number of true positives is 91/127\n",
    "      * Of those 91 true positive terms, about 41 are terms which are part of a taxonomic phrase (i.e. - either only the genus part of a taxonomy or a species part\n",
    "      * This means that only about 41 of these terms would be missed with the number of letters threshhold for EXTRACT were >5, the remaining true positives would likely be captured with the whole term\n",
    "2. Text2Term is also weaker at mapping terms formatted as g. species, as it can score higher mapping to the correct species term but incorrect genus. To address this:\n",
    "    * Identify such terms using regex (r\"\\b[A-Z]\\.\\s[^\\s]+\\b\")\n",
    "    * For these terms, only take the result if the genus letter matches the first letter of the mapped result (i.e. split on '.', take first)\n",
    "    * There will be plenty of exceptions, but this should address the majority of problematic matches\n",
    "3. Default behavior if prior conditions don't apply:\n",
    "    * Sort by score (highest to lowest), keep first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74fc4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
