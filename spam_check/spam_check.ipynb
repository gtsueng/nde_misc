{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a90a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587af842",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.getcwd()\n",
    "dump_path = os.path.join(script_path,'gpt_flagged_dumps')\n",
    "manual_dump_path = os.path.join(script_path,'manually_flagged_spam')\n",
    "term_dump_path = os.path.join(script_path,'term_based_spam')\n",
    "data_path = os.path.join(script_path,'data')\n",
    "spamlist_file = os.path.join(data_path,'GPT-categorized-ads.xlsx')\n",
    "repo_dumps = os.path.join(script_path,'dumps_from_repos')\n",
    "manual_spam_file = os.path.join(data_path,'manually_identified_spam.xlsx')\n",
    "\n",
    "zenodo_spam = pd.read_excel(spamlist_file, 'Zenodo', engine='openpyxl', header=0)\n",
    "mendeley_spam = pd.read_excel(spamlist_file, 'Mendeley', engine='openpyxl', header=0)\n",
    "manually_found_spam = pd.read_excel(manual_spam_file, 'spam', engine='openpyxl', header=0)\n",
    "spam_search_terms = pd.read_excel(manual_spam_file, 'spamterms', engine='openpyxl', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26eeef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    resource_id repository                  resource_url  \\\n",
      "0  dataverse_10.7910_dvn_hpqe8t  dataverse  dataverse_10.7910_dvn_hpqe8t   \n",
      "1  dataverse_10.7910_dvn_eowrl9  dataverse  dataverse_10.7910_dvn_eowrl9   \n",
      "\n",
      "                           access_url  \n",
      "0  https://doi.org/10.7910/DVN/HPQE8T  \n",
      "1  https://doi.org/10.7910/DVN/EOWRL9  \n",
      "           phrases\n",
      "0  discount coupon\n",
      "1             keto\n"
     ]
    }
   ],
   "source": [
    "print(manually_found_spam.head(n=2))\n",
    "print(spam_search_terms.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9278cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenodo_ids = zenodo_spam['id'].unique().tolist()\n",
    "clean_zenids = [x.replace('ZENODO_','') for x in zenodo_ids]\n",
    "print(clean_zenids[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e7b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Download all records flagged as spam\n",
    "def download_gpt_flagged_spam(dump_path, spam_ids):\n",
    "    repo_deleted = []\n",
    "    nde_missing = []\n",
    "    for eachid in spam_ids:\n",
    "        cleanid = eachid.lower()\n",
    "        if \"zenodo\" in cleanid:\n",
    "            repo = \"zenodo\"\n",
    "        elif \"mendeley\" in cleanid:\n",
    "            repo = \"mendeley\"\n",
    "        elif \"dataverse\" in cleanid:\n",
    "            repo = \"dataverse\"\n",
    "        nde_api_url = f\"https://api-staging.data.niaid.nih.gov/v1/query?&q={cleanid}\"\n",
    "        r = requests.get(nde_api_url)\n",
    "        if r.status_code == 200:\n",
    "            tmp = json.loads(r.text)\n",
    "            if len(tmp['hits'])>0:\n",
    "                tmphit = tmp['hits'][0]\n",
    "                with open(os.path.join(dump_path,f\"{cleanid}.json\"),'w') as outfile:\n",
    "                    outfile.write(json.dumps(tmphit, indent=4))\n",
    "        else:\n",
    "            nde_missing.append(cleanid)\n",
    "        if repo == \"zenodo\":\n",
    "            zenurlid = cleanid.replace(\"zenodo_\",\"\")\n",
    "            zenrequest = requests.get(f\"https://zenodo.org/api/records/{zenurlid}\")\n",
    "            if zenrequest.status_code == 410:\n",
    "                repo_deleted.append(cleanid)\n",
    "            time.sleep(1.125)\n",
    "        elif repo == \"mendeley\":\n",
    "            mendeley_id = cleanid.replace(\"mendeley_\",\"\")\n",
    "            mendeleyrequest = requests.get(f\"https://data.mendeley.com/oai?verb=GetRecord&metadataPrefix=datacite&identifier=oai:data.mendeley.com/{mendeley_id}\")\n",
    "            if \"idDoesNotExist\" in mendeleyrequest.text:\n",
    "                repo_deleted.append(cleanid)\n",
    "                time.sleep(1.125)                \n",
    "\n",
    "    today = datetime.now()\n",
    "    with open(os.path.join(repo_dumps,f'repo_deletions_{today.strftime(\"%Y-%m-%d\")}.txt'),'w') as dumpfile:\n",
    "        for eachrecord in repo_deleted:\n",
    "            dumpfile.write(eachrecord+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb2c0d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 35.9 s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mendeley_ids = mendeley_spam['id'].unique().tolist()\n",
    "download_gpt_flagged_spam(dump_path, mendeley_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ed04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9d68b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_spam_records(manual_dump_path,idlist):\n",
    "    repo_deleted = []\n",
    "    nde_missing = []\n",
    "    repo_urlist = []\n",
    "    for eachid in idlist:\n",
    "        nde_api_url = f\"https://api-staging.data.niaid.nih.gov/v1/query?&q={eachid}\"\n",
    "        r = requests.get(nde_api_url)\n",
    "        if r.status_code == 200:\n",
    "            tmp = json.loads(r.text)\n",
    "            if len(tmp['hits'])>0:\n",
    "                tmphit = tmp['hits'][0]\n",
    "                with open(os.path.join(manual_dump_path,f\"{eachid}.json\"),'w') as outfile:\n",
    "                    outfile.write(json.dumps(tmphit, indent=4))\n",
    "                repo_urlist.append(tmphit['url'])\n",
    "    return repo_urlist\n",
    "\n",
    "def search_for_spam(term_dump_path,spamterms):\n",
    "    greirepos = [\"Zenodo\",\"Mendeley\",\"Harvard Dataverse\"]\n",
    "    summary = []\n",
    "    for eachterm in spamterms:\n",
    "        for eachrepo in greirepos:\n",
    "            nde_search_url = f'https://api-staging.data.niaid.nih.gov/v1/query?&q=includedInDataCatalog.name:\"{eachrepo}\"+AND+\"{eachterm}\"&fetch_all=true'\n",
    "            r = requests.get(nde_search_url)\n",
    "            tmp = json.loads(r.text)\n",
    "            if len(tmp['hits'])>0:\n",
    "                for eachhit in tmp['hits']:\n",
    "                    tmpid = eachhit['_id']\n",
    "                    with open(os.path.join(term_dump_path,f\"{tmpid}.json\"),'w') as outfile:\n",
    "                        outfile.write(json.dumps(eachhit, indent=4))\n",
    "                    summary.append({\"repo\":eachrepo,\"search_phrase\":eachterm,\"id\":eachhit['_id'],\"data_url\":eachhit['url']})\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cffe22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.17 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "idlist = manually_found_spam['resource_id'].unique().tolist()\n",
    "repo_url_list = download_spam_records(manual_dump_path,idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc785062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.53 s\n",
      "Wall time: 8.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spamterms = spam_search_terms[\"phrases\"].unique().tolist()\n",
    "summary_df = search_for_spam(term_dump_path,spamterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c80b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv(os.path.join(data_path,'search_based_spam.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64278e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_repo_url(repo_url_list):\n",
    "    failed_urls = []\n",
    "    for eachurl in repo_url_list:\n",
    "        if 'zenodo' in eachurl:\n",
    "            r = requests.get(eachurl)\n",
    "            if r.status_code == 410:\n",
    "                failed_urls.append(eachurl)\n",
    "        elif 'DVN' in eachurl:\n",
    "            r = requests.get(eachurl)\n",
    "            if r.status_code == 404:\n",
    "                failed_urls.append(eachurl)\n",
    "        elif 'mendeley' in eachurl:\n",
    "            r = requests.get(eachurl)\n",
    "            if \"Dataset Not Found\" in r.text:\n",
    "                failed_urls.append(eachurl)\n",
    "        time.sleep(1)\n",
    "    return failed_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d8b25",
   "metadata": {},
   "source": [
    "### Test for deleted record or broken link status codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1a453a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://zenodo.org/record/12778828\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45153ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2TAGUN\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2484cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "#r = requests.get(\"https://data.mendeley.com/datasets/8kj2rn7m9z\")\n",
    "r = requests.get(\"https://data.mendeley.com/datasets/8kj2rn7m9x\")\n",
    "print(r.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab220249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                repo    search_phrase                            id  \\\n",
      "0  Harvard Dataverse  discount coupon  dataverse_10.7910_dvn_2taguy   \n",
      "1             Zenodo             keto               zenodo_10575214   \n",
      "\n",
      "                             data_url  \n",
      "0  https://doi.org/10.7910/DVN/2TAGUY  \n",
      "1  https://zenodo.org/record/10575214  \n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.read_csv(os.path.join(data_path,'search_based_spam.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "print(summary_df.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "471d0403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "CPU times: total: 47.8 s\n",
      "Wall time: 20min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "repo_url_list = summary_df['data_url'].unique().tolist()\n",
    "deleted_records = test_repo_url(repo_url_list)\n",
    "\n",
    "with open(os.path.join(data_path,'deleted_records.txt'),'w') as outwrite:\n",
    "    for eachrecord in deleted_records:\n",
    "        outwrite.write(eachrecord+'\\n')\n",
    "print(len(deleted_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f25ba7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dumplist = os.listdir(dump_path)\n",
    "manual_dumplist = os.listdir(manual_dump_path)\n",
    "\n",
    "spam_urls = []\n",
    "for eachjson in gpt_dumplist:\n",
    "    with open(os.path.join(dump_path,eachjson),'r') as infile:\n",
    "        tmpjson = json.load(infile)\n",
    "        spam_urls.append(tmpjson['url'])\n",
    "for eachjson in manual_dumplist:\n",
    "    with open(os.path.join(manual_dump_path,eachjson),'r') as infile:\n",
    "        tmpjson = json.load(infile)\n",
    "        spam_urls.append(tmpjson['url'])\n",
    "\n",
    "not_checked = [x for x in spam_urls if x not in repo_url_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca95ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "more_deleted = test_repo_url(spam_urls)\n",
    "with open(os.path.join(data_path,'deleted_records.txt'),'a') as outwrite:\n",
    "    for eachrecord in more_deleted:\n",
    "        outwrite.write(eachrecord+'\\n')\n",
    "print(len(deleted_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b4be4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5404c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
